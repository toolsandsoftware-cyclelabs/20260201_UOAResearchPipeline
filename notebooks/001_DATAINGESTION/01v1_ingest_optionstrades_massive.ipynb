{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "01v1_ingest_optionstrades_massive.ipynb\n",
    "================================================================================\n",
    "Options Trade-by-Trade Data Ingestion from Massive S3 Files\n",
    "\n",
    "DESCRIPTION\n",
    "-----------\n",
    "Downloads raw options trade data from Massive's S3-compatible storage and saves\n",
    "it as daily Parquet files. Each file contains ALL options trades for ALL \n",
    "optionable tickers for a single trading day.\n",
    "\n",
    "CONFIGURATION\n",
    "-------------\n",
    "Set these variables before running:\n",
    "\n",
    "    ACCESS_KEY   : str  - Massive S3 access key\n",
    "    SECRET_KEY   : str  - Massive S3 secret key\n",
    "    START_DATE   : str  - Start date in \"YYYY-MM-DD\" format\n",
    "    END_DATE     : str  - End date in \"YYYY-MM-DD\" format\n",
    "    OUTPUT_DIR   : str  - Directory path for output files (default: \"./ALL_TRADES\")\n",
    "    MAX_WORKERS  : int  - Number of parallel workers (default: 10)\n",
    "\n",
    "OUTPUT\n",
    "------\n",
    "Parquet files saved to OUTPUT_DIR with naming convention:\n",
    "\n",
    "    {OUTPUT_DIR}/YYYY-MM-DD.parquet\n",
    "\n",
    "Each file contains trades with the following columns:\n",
    "\n",
    "    ticker         : str  - Option symbol (e.g., \"O:TSLA240119C00250000\")\n",
    "                            Format: O:{UNDERLYING}{YYMMDD}{C/P}{STRIKE*1000}\n",
    "    conditions     : int  - Trade condition code (see Massive docs)\n",
    "    correction     : int  - Correction indicator (0 = no correction)\n",
    "    exchange       : int  - Exchange code where trade occurred\n",
    "    price          : float - Trade price per share (multiply by 100 for contract value)\n",
    "    sip_timestamp  : str  - Unix timestamp in nanoseconds (UTC)\n",
    "    size           : int  - Number of contracts traded\n",
    "    underlying     : str  - Underlying ticker symbol (e.g., \"TSLA\")\n",
    "\n",
    "NOTES\n",
    "-----\n",
    "- Only processes business days (Monday-Friday)\n",
    "- Skips dates that already have output files (idempotent)\n",
    "- Holidays and unavailable dates are logged and skipped gracefully\n",
    "- Data typically available ~1 hour after market close\n",
    "\n",
    "USAGE\n",
    "-----\n",
    "1. Set ACCESS_KEY and SECRET_KEY with your Massive credentials\n",
    "2. Set START_DATE and END_DATE for desired date range\n",
    "3. Run the script\n",
    "\n",
    "EXAMPLE\n",
    "-------\n",
    "    ACCESS_KEY = \"your_access_key\"\n",
    "    SECRET_KEY = \"your_secret_key\"\n",
    "    START_DATE = \"2024-01-01\"\n",
    "    END_DATE   = \"2024-01-31\"\n",
    "    OUTPUT_DIR = \"./ALL_TRADES\"\n",
    "    \n",
    "    # Then run the script\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "ACCESS_KEY = \"\"\n",
    "SECRET_KEY = \"\"\n",
    "\n",
    "BUCKET_NAME = \"flatfiles\"\n",
    "S3_ENDPOINT = \"https://files.massive.com\"\n",
    "\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE   = \"2020-01-20\"\n",
    "\n",
    "OUTPUT_DIR = \"./ALL_TRADES\"  # Changed to reflect all tickers\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "# ======================================================\n",
    "\n",
    "\n",
    "def get_s3_client():\n",
    "    \"\"\"Create a Massive.com-compatible S3 client.\"\"\"\n",
    "    return boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=S3_ENDPOINT,\n",
    "        region_name=\"us-east-1\",\n",
    "        aws_access_key_id=ACCESS_KEY,\n",
    "        aws_secret_access_key=SECRET_KEY,\n",
    "        config=Config(\n",
    "            signature_version=\"s3v4\",\n",
    "            retries={\"max_attempts\": 3},\n",
    "            s3={\"addressing_style\": \"path\"}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Yield business days between start and end date.\"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    for n in range((end - start).days + 1):\n",
    "        dt = start + timedelta(days=n)\n",
    "        if dt.weekday() < 5:  # Monday–Friday\n",
    "            yield dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def download_day(date: str):\n",
    "    filename = f\"{date}.csv.gz\"\n",
    "    year = date[:4]\n",
    "    month = date[5:7]\n",
    "    s3_key = f\"us_options_opra/trades_v1/{year}/{month}/{filename}\"\n",
    "\n",
    "    temp_path = os.path.join(tempfile.gettempdir(), filename)\n",
    "\n",
    "    # Early skip: if the daily file already exists, don't download\n",
    "    outpath = os.path.join(OUTPUT_DIR, f\"{date}.parquet\")\n",
    "    if os.path.exists(outpath):\n",
    "        return f\"✓ {date}: already processed\"\n",
    "\n",
    "    try:\n",
    "        s3 = get_s3_client()\n",
    "        s3.download_file(BUCKET_NAME, s3_key, temp_path)\n",
    "\n",
    "        # Read the CSV with actual column names\n",
    "        df = pd.read_csv(\n",
    "            temp_path,\n",
    "            compression=\"gzip\",\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Find the ticker column (should be 'ticker' based on the schema)\n",
    "        ticker_col = None\n",
    "        possible_ticker_cols = [\"ticker\", \"underlying_ticker\", \"underlying\", \"symbol\"]\n",
    "        \n",
    "        for col in possible_ticker_cols:\n",
    "            if col in df.columns:\n",
    "                ticker_col = col\n",
    "                break\n",
    "        \n",
    "        if ticker_col is None:\n",
    "            return f\"✗ {date}: no ticker column found. Available: {list(df.columns)}\"\n",
    "        \n",
    "        # Extract underlying ticker from option symbols (format: O:SPY230324C00450000)\n",
    "        # The underlying is between \"O:\" and the date\n",
    "        def extract_underlying(option_symbol):\n",
    "            try:\n",
    "                if pd.isna(option_symbol):\n",
    "                    return None\n",
    "                s = str(option_symbol)\n",
    "                if s.startswith(\"O:\"):\n",
    "                    # Remove \"O:\" prefix\n",
    "                    s = s[2:]\n",
    "                    # Find where the date starts (first digit after ticker letters)\n",
    "                    for i, char in enumerate(s):\n",
    "                        if char.isdigit():\n",
    "                            return s[:i]\n",
    "                return s\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        df['underlying'] = df[ticker_col].apply(extract_underlying)\n",
    "        \n",
    "        # Debug: show what underlyings we found (optional, can remove if not needed)\n",
    "        # unique_underlyings = df['underlying'].unique()[:20]\n",
    "        \n",
    "        if df.empty:\n",
    "            return f\"− {date}: no trades\"\n",
    "\n",
    "        # Save the full DataFrame\n",
    "        df.to_parquet(outpath, compression=\"snappy\")\n",
    "\n",
    "        size_mb = os.path.getsize(outpath) / 1e6\n",
    "        return f\"✓ {date}: {len(df):,} trades ({size_mb:.2f} MB)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        # Normal behavior: Massive returns 403 for \"file not found\"\n",
    "        if \"Forbidden\" in msg or \"404\" in msg or \"NoSuchKey\" in msg:\n",
    "            return f\"− {date}: no file (holiday or unavailable)\"\n",
    "        return f\"✗ {date}: {msg[:200]}\"\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(temp_path):\n",
    "            try:\n",
    "                os.remove(temp_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "# ==================== MAIN ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dates = list(daterange(START_DATE, END_DATE))\n",
    "\n",
    "    print(f\"Downloading options trades (all tickers)\")\n",
    "    print(f\"Date range : {START_DATE} → {END_DATE}\")\n",
    "    print(f\"Days       : {len(dates)} business days\")\n",
    "    print(f\"Workers    : {MAX_WORKERS}\")\n",
    "    print(f\"Output     : {OUTPUT_DIR}\\n\")\n",
    "\n",
    "    # Process dates sequentially for easier debugging\n",
    "    for date in tqdm(dates, desc=\"Progress\"):\n",
    "        result = download_day(date)\n",
    "        print(result)\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    print(\"=\" * 60)\n",
    "    files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(\".parquet\")]\n",
    "    total_gb = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) for f in files) / 1e9\n",
    "    print(f\"  {len(files)} files, {total_gb:.2f} GB\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
