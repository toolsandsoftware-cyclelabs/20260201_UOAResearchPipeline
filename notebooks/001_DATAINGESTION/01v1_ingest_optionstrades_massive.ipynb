{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "001_Data Ingestion >>> 01v1_ingest_optionstrades_massive.ipynb\n",
    "\n",
    "\n",
    "This is the final code to ingest options trades-by-trade data from Massive S3 Files.\n",
    "\n",
    "Inputs include the following: \n",
    "---> ACCESS_KEY\n",
    "---> SECRET_KEY\n",
    "---> START_DATE\n",
    "---> END_DATE\n",
    "---> OUTPUT_DIR\n",
    "---> MAX_WORKERS\n",
    "\n",
    "The output is a parquet file (or files).\n",
    "---> Each file is named after the trading date with parquet as the extension.\n",
    "---> Example filename is 2025-11-21.parquet\n",
    "---> Each file contains all options trades for ALL optionable tickers for that day. \n",
    "\n",
    "The columns and format of each data are the following:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "ACCESS_KEY = \"\"\n",
    "SECRET_KEY = \"\"\n",
    "\n",
    "BUCKET_NAME = \"flatfiles\"\n",
    "S3_ENDPOINT = \"https://files.massive.com\"\n",
    "\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE   = \"2020-01-20\"\n",
    "\n",
    "OUTPUT_DIR = \"./ALL_TRADES\"  # Changed to reflect all tickers\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "# ======================================================\n",
    "\n",
    "\n",
    "def get_s3_client():\n",
    "    \"\"\"Create a Massive.com-compatible S3 client.\"\"\"\n",
    "    return boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=S3_ENDPOINT,\n",
    "        region_name=\"us-east-1\",\n",
    "        aws_access_key_id=ACCESS_KEY,\n",
    "        aws_secret_access_key=SECRET_KEY,\n",
    "        config=Config(\n",
    "            signature_version=\"s3v4\",\n",
    "            retries={\"max_attempts\": 3},\n",
    "            s3={\"addressing_style\": \"path\"}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Yield business days between start and end date.\"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    for n in range((end - start).days + 1):\n",
    "        dt = start + timedelta(days=n)\n",
    "        if dt.weekday() < 5:  # Monday–Friday\n",
    "            yield dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def download_day(date: str):\n",
    "    filename = f\"{date}.csv.gz\"\n",
    "    year = date[:4]\n",
    "    month = date[5:7]\n",
    "    s3_key = f\"us_options_opra/trades_v1/{year}/{month}/{filename}\"\n",
    "\n",
    "    temp_path = os.path.join(tempfile.gettempdir(), filename)\n",
    "\n",
    "    # Early skip: if the daily file already exists, don't download\n",
    "    outpath = os.path.join(OUTPUT_DIR, f\"{date}.parquet\")\n",
    "    if os.path.exists(outpath):\n",
    "        return f\"✓ {date}: already processed\"\n",
    "\n",
    "    try:\n",
    "        s3 = get_s3_client()\n",
    "        s3.download_file(BUCKET_NAME, s3_key, temp_path)\n",
    "\n",
    "        # Read the CSV with actual column names\n",
    "        df = pd.read_csv(\n",
    "            temp_path,\n",
    "            compression=\"gzip\",\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Find the ticker column (should be 'ticker' based on the schema)\n",
    "        ticker_col = None\n",
    "        possible_ticker_cols = [\"ticker\", \"underlying_ticker\", \"underlying\", \"symbol\"]\n",
    "        \n",
    "        for col in possible_ticker_cols:\n",
    "            if col in df.columns:\n",
    "                ticker_col = col\n",
    "                break\n",
    "        \n",
    "        if ticker_col is None:\n",
    "            return f\"✗ {date}: no ticker column found. Available: {list(df.columns)}\"\n",
    "        \n",
    "        # Extract underlying ticker from option symbols (format: O:SPY230324C00450000)\n",
    "        # The underlying is between \"O:\" and the date\n",
    "        def extract_underlying(option_symbol):\n",
    "            try:\n",
    "                if pd.isna(option_symbol):\n",
    "                    return None\n",
    "                s = str(option_symbol)\n",
    "                if s.startswith(\"O:\"):\n",
    "                    # Remove \"O:\" prefix\n",
    "                    s = s[2:]\n",
    "                    # Find where the date starts (first digit after ticker letters)\n",
    "                    for i, char in enumerate(s):\n",
    "                        if char.isdigit():\n",
    "                            return s[:i]\n",
    "                return s\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        df['underlying'] = df[ticker_col].apply(extract_underlying)\n",
    "        \n",
    "        # Debug: show what underlyings we found (optional, can remove if not needed)\n",
    "        # unique_underlyings = df['underlying'].unique()[:20]\n",
    "        \n",
    "        if df.empty:\n",
    "            return f\"− {date}: no trades\"\n",
    "\n",
    "        # Save the full DataFrame\n",
    "        df.to_parquet(outpath, compression=\"snappy\")\n",
    "\n",
    "        size_mb = os.path.getsize(outpath) / 1e6\n",
    "        return f\"✓ {date}: {len(df):,} trades ({size_mb:.2f} MB)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        # Normal behavior: Massive returns 403 for \"file not found\"\n",
    "        if \"Forbidden\" in msg or \"404\" in msg or \"NoSuchKey\" in msg:\n",
    "            return f\"− {date}: no file (holiday or unavailable)\"\n",
    "        return f\"✗ {date}: {msg[:200]}\"\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(temp_path):\n",
    "            try:\n",
    "                os.remove(temp_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "# ==================== MAIN ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dates = list(daterange(START_DATE, END_DATE))\n",
    "\n",
    "    print(f\"Downloading options trades (all tickers)\")\n",
    "    print(f\"Date range : {START_DATE} → {END_DATE}\")\n",
    "    print(f\"Days       : {len(dates)} business days\")\n",
    "    print(f\"Workers    : {MAX_WORKERS}\")\n",
    "    print(f\"Output     : {OUTPUT_DIR}\\n\")\n",
    "\n",
    "    # Process dates sequentially for easier debugging\n",
    "    for date in tqdm(dates, desc=\"Progress\"):\n",
    "        result = download_day(date)\n",
    "        print(result)\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    print(\"=\" * 60)\n",
    "    files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(\".parquet\")]\n",
    "    total_gb = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) for f in files) / 1e9\n",
    "    print(f\"  {len(files)} files, {total_gb:.2f} GB\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
