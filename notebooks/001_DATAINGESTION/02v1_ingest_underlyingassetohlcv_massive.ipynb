{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1889f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "Phase 01 - Data Ingestion: Underlying Asset Daily Aggregates\n",
    "================================================================================\n",
    "\n",
    "PIPELINE POSITION:\n",
    "    Phase 01 (Data Ingestion) → Component B: Underlying Asset Data\n",
    "    Part of the UOA Research Pipeline for institutional options flow analysis.\n",
    "\n",
    "PURPOSE:\n",
    "    Downloads daily stock aggregates (OHLCV) from Massive S3 and converts to\n",
    "    Parquet format. This data provides the underlying price context needed for\n",
    "    options trade analysis in subsequent pipeline phases.\n",
    "\n",
    "DATA SOURCE:\n",
    "    Massive S3: s3://flatfiles/us_stocks_sip/day_aggs_v1/{YYYY}/{MM}/{YYYY-MM-DD}.csv.gz\n",
    "\n",
    "INPUT:\n",
    "    - Gzipped CSV files from Massive S3 (one per trading day)\n",
    "    - Date range: START_DATE to END_DATE (configurable)\n",
    "\n",
    "OUTPUT:\n",
    "    - Directory: ./US_STOCKS_DAY_AGGS/\n",
    "    - Format: {YYYY-MM-DD}.parquet (Snappy compression)\n",
    "    - One file per trading day, all tickers combined\n",
    "\n",
    "OUTPUT SCHEMA:\n",
    "    ticker        : str   - Exchange symbol (e.g., \"AAPL\")\n",
    "    volume        : int   - Trading volume for the day\n",
    "    open          : float - Open price \n",
    "    close         : float - Close price (UNADJUSTED)\n",
    "    high          : float - High price\n",
    "    low           : float - Low price\n",
    "    window_start  : int   - Unix nanosecond timestamp (market open)\n",
    "    transactions  : int   - Number of transactions\n",
    "\n",
    "NOTES:\n",
    "    - Close prices are UNADJUSTED (no split/dividend adjustments)\n",
    "    - Skips weekends; returns \"no file\" for market holidays\n",
    "    - Idempotent: skips dates with existing .parquet files\n",
    "    - Massive returns 403 for missing files (treated as holiday/unavailable)\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "ACCESS_KEY = \"\"\n",
    "SECRET_KEY = \"\"\n",
    "\n",
    "BUCKET_NAME = \"flatfiles\"\n",
    "S3_ENDPOINT = \"https://files.massive.com\"\n",
    "\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE   = \"2025-12-31\"\n",
    "\n",
    "OUTPUT_DIR = \"./US_STOCKS_DAY_AGGS\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "# ======================================================\n",
    "\n",
    "\n",
    "def get_s3_client():\n",
    "    \"\"\"Create a Massive.com-compatible S3 client.\"\"\"\n",
    "    return boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=S3_ENDPOINT,\n",
    "        region_name=\"us-east-1\",\n",
    "        aws_access_key_id=ACCESS_KEY,\n",
    "        aws_secret_access_key=SECRET_KEY,\n",
    "        config=Config(\n",
    "            signature_version=\"s3v4\",\n",
    "            retries={\"max_attempts\": 3},\n",
    "            s3={\"addressing_style\": \"path\"}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Yield business days between start and end date.\"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    for n in range((end - start).days + 1):\n",
    "        dt = start + timedelta(days=n)\n",
    "        if dt.weekday() < 5:  # Monday–Friday\n",
    "            yield dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def download_day(date: str):\n",
    "    filename = f\"{date}.csv.gz\"\n",
    "    year = date[:4]\n",
    "    month = date[5:7]\n",
    "    s3_key = f\"us_stocks_sip/day_aggs_v1/{year}/{month}/{filename}\"\n",
    "\n",
    "    temp_path = os.path.join(tempfile.gettempdir(), filename)\n",
    "\n",
    "    # Early skip: if the daily file already exists, don't download\n",
    "    outpath = os.path.join(OUTPUT_DIR, f\"{date}.parquet\")\n",
    "    if os.path.exists(outpath):\n",
    "        return f\"✓ {date}: already processed\"\n",
    "\n",
    "    try:\n",
    "        s3 = get_s3_client()\n",
    "        s3.download_file(BUCKET_NAME, s3_key, temp_path)\n",
    "\n",
    "        # Read the CSV with actual column names\n",
    "        df = pd.read_csv(\n",
    "            temp_path,\n",
    "            compression=\"gzip\",\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        if df.empty:\n",
    "            return f\"− {date}: no data\"\n",
    "\n",
    "        # Save the full DataFrame (no need for underlying extraction as this is stock data)\n",
    "        df.to_parquet(outpath, compression=\"snappy\")\n",
    "\n",
    "        size_mb = os.path.getsize(outpath) / 1e6\n",
    "        return f\"✓ {date}: {len(df):,} rows ({size_mb:.2f} MB)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        # Normal behavior: Massive returns 403 for \"file not found\"\n",
    "        if \"Forbidden\" in msg or \"404\" in msg or \"NoSuchKey\" in msg:\n",
    "            return f\"− {date}: no file (holiday or unavailable)\"\n",
    "        return f\"✗ {date}: {msg[:200]}\"\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(temp_path):\n",
    "            try:\n",
    "                os.remove(temp_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "# ==================== MAIN ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dates = list(daterange(START_DATE, END_DATE))\n",
    "\n",
    "    print(f\"Downloading US stocks day aggregates\")\n",
    "    print(f\"Date range : {START_DATE} → {END_DATE}\")\n",
    "    print(f\"Days       : {len(dates)} business days\")\n",
    "    print(f\"Workers    : {MAX_WORKERS}\")\n",
    "    print(f\"Output     : {OUTPUT_DIR}\\n\")\n",
    "\n",
    "    # Process dates sequentially for easier debugging\n",
    "    for date in tqdm(dates, desc=\"Progress\"):\n",
    "        result = download_day(date)\n",
    "        print(result)\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    print(\"=\" * 60)\n",
    "    files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(\".parquet\")]\n",
    "    total_gb = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) for f in files) / 1e9\n",
    "    print(f\"  {len(files)} files, {total_gb:.2f} GB\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
