{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "========================================================================================\n",
    "PHASE 04A-2: AGGREGATED FEATURE ENGINEERING FOR TYPE A DETECTION [LARGE INFORMED TRADE]\n",
    "========================================================================================\n",
    "\n",
    "Pipeline Position: \n",
    "    Phase 03 (Per-Row Features) → Phase 04A-1 (Baselines) → **Phase 04A-2**\n",
    "    → Phase 05 (Anomaly Detection)\n",
    "\n",
    "Purpose:\n",
    "    Transforms per-trade data into feature-rich records optimized for detecting\n",
    "    Type A unusual options activity (large informed single trades). Computes\n",
    "    aggregations at multiple levels (contract, expiry, chain) and compares\n",
    "    against historical baselines to identify statistically unusual activity.\n",
    "\n",
    "Target Signal (Type A Characteristics):\n",
    "    - Large trade size relative to contract/chain\n",
    "    - Paying above VWAP (urgency)\n",
    "    - OTM strikes (leverage seeking)\n",
    "    - Near-dated expiries (catalyst-driven)\n",
    "    - Concentrated in specific contracts\n",
    "    - Elevated chain volume vs historical baseline\n",
    "\n",
    "Input:\n",
    "    - Phase 03 output files: {TICKER}_perrowfeatures_YYYY-MM-DD.parquet\n",
    "    - Phase 04A-1 baseline table: baseline_{TICKER}_daily.parquet\n",
    "    \n",
    "Output:\n",
    "    - Feature-enriched files: {TICKER}_aggregatedfeatures_YYYY-MM-DD.parquet\n",
    "    - Each row is a single trade with ~72 additional aggregated features\n",
    "\n",
    "Author: [Your Name]\n",
    "Created: 2026-02-XX\n",
    "Version: 1.0\n",
    "\n",
    "Dependencies:\n",
    "    - pandas >= 1.5.0\n",
    "    - numpy >= 1.20.0\n",
    "    - pyarrow >= 10.0.0\n",
    "\n",
    "Usage:\n",
    "    python \"scriptname.py\"\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Input folder containing Phase 03A output files\n",
    "    \"input_folder\": Path(r\"D:\\cyclelabs_codes\\CL_20251120_siphontrades\\01_FIXINGRAWDATA\\output_2_perrowfeateng\"),\n",
    "    \n",
    "    # Baseline folder from Phase 03B-1 (contains baseline_{TICKER}_daily.parquet files)\n",
    "    \"baseline_folder\": Path(r\"D:\\cyclelabs_codes\\CL_20251120_siphontrades\\01_FIXINGRAWDATA\\output_4a1_baseline\"),\n",
    "    \n",
    "    # Output folder for aggregated features\n",
    "    \"output_folder\": Path(r\"D:\\cyclelabs_codes\\CL_20251120_siphontrades\\01_FIXINGRAWDATA\\output_4a2_aggfeateng\"),\n",
    "    \n",
    "    # Date range to process (inclusive)\n",
    "    \"start_date\": \"2024-02-01\",\n",
    "    \"end_date\": \"2025-12-31\",\n",
    "    \n",
    "    # Tickers to process (None or [] for all tickers)\n",
    "    \"tickers_to_process\": [\"CIFR\"],  # e.g., [\"AAPL\", \"TSLA\", \"CIFR\"]\n",
    "    \n",
    "    # OTM percentage thresholds\n",
    "    \"otm_threshold\": 5.0,        # > 5% = OTM\n",
    "    \"deep_otm_threshold\": 20.0,  # > 20% = Deep OTM\n",
    "    \n",
    "    # DTE thresholds\n",
    "    \"short_dte_threshold\": 14,   # <= 14 days = short-dated\n",
    "    \"medium_dte_threshold\": 45,  # <= 45 days = medium-dated\n",
    "    \n",
    "    # Minimum thresholds for valid signals\n",
    "    \"min_chain_volume\": 100,          # Minimum chain volume\n",
    "    \"min_chain_trades\": 10,           # Minimum trade count\n",
    "    \"min_contract_trades\": 1,         # Minimum trades per contract\n",
    "    \n",
    "    # Z-score clipping for composite scores\n",
    "    \"zscore_clip_max\": 5.0,\n",
    "    \n",
    "    # Epsilon for division safety\n",
    "    \"epsilon\": 1e-8,\n",
    "    \n",
    "    # Logging level\n",
    "    \"log_level\": logging.INFO,\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# LOGGING SETUP\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=CONFIG[\"log_level\"],\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def parse_filename(filename: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Parse ticker and date from Phase 03A output filename.\n",
    "    \n",
    "    Expected format: {TICKER}_perrowfeatures_YYYY-MM-DD.parquet\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base = filename.replace('.parquet', '')\n",
    "        parts = base.split('_perrowfeatures_')\n",
    "        if len(parts) != 2:\n",
    "            return None, None\n",
    "        ticker = parts[0]\n",
    "        date_str = parts[1]\n",
    "        datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        return ticker, date_str\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def safe_divide(numerator, denominator, default=0.0):\n",
    "    \"\"\"Safe division handling zero denominators. Works with scalars and arrays.\"\"\"\n",
    "    if isinstance(numerator, (pd.Series, np.ndarray)):\n",
    "        result = np.where(\n",
    "            (denominator == 0) | pd.isna(denominator),\n",
    "            default,\n",
    "            numerator / (denominator + CONFIG[\"epsilon\"])\n",
    "        )\n",
    "        return result\n",
    "    else:\n",
    "        if denominator == 0 or pd.isna(denominator):\n",
    "            return default\n",
    "        return numerator / denominator\n",
    "\n",
    "\n",
    "def compute_hhi(values: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Compute Herfindahl-Hirschman Index for concentration measurement.\n",
    "    \"\"\"\n",
    "    total = values.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    shares = values / total\n",
    "    return (shares ** 2).sum()\n",
    "\n",
    "\n",
    "def compute_zscore(value, mean, std):\n",
    "    \"\"\"Compute z-score with handling for zero/nan std.\"\"\"\n",
    "    if isinstance(value, (pd.Series, np.ndarray)):\n",
    "        result = np.where(\n",
    "            (std == 0) | pd.isna(std) | pd.isna(mean),\n",
    "            0.0,\n",
    "            (value - mean) / (std + CONFIG[\"epsilon\"])\n",
    "        )\n",
    "        return result\n",
    "    else:\n",
    "        if std == 0 or pd.isna(std) or pd.isna(mean):\n",
    "            return 0.0\n",
    "        return (value - mean) / std\n",
    "\n",
    "\n",
    "def normalize_series(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize series to 0-1 range.\"\"\"\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    if max_val == min_val:\n",
    "        return pd.Series(0.5, index=series.index)\n",
    "    return (series - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def clip_zscore(zscore, max_val=None):\n",
    "    \"\"\"Clip z-score to reasonable range.\"\"\"\n",
    "    if max_val is None:\n",
    "        max_val = CONFIG[\"zscore_clip_max\"]\n",
    "    if isinstance(zscore, (pd.Series, np.ndarray)):\n",
    "        return np.clip(zscore, -max_val, max_val)\n",
    "    return max(-max_val, min(max_val, zscore))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE GROUP 1: CONTRACT-LEVEL AGGREGATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_contract_level_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute contract-level aggregations and join back to trades.\n",
    "    \n",
    "    Contract = same strike + expiry + type (identified by 'ticker' column)\n",
    "    \"\"\"\n",
    "    logger.debug(\"  Computing contract-level features...\")\n",
    "    \n",
    "    eps = CONFIG[\"epsilon\"]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Aggregate statistics per contract\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    contract_agg = df.groupby(\"ticker\").agg(\n",
    "        contract_total_volume=(\"size\", \"sum\"),\n",
    "        contract_total_notional=(\"opt_trade_notional_value\", \"sum\"),\n",
    "        contract_trade_count=(\"size\", \"count\"),\n",
    "        contract_max_size=(\"size\", \"max\"),\n",
    "        contract_min_size=(\"size\", \"min\"),\n",
    "        contract_mean_size=(\"size\", \"mean\"),\n",
    "        contract_oi=(\"open_interest_now\", \"first\"),\n",
    "        contract_oi_yesterday=(\"open_interest_yesterday\", \"first\"),\n",
    "        contract_price_sum_product=(\"price\", lambda x: (x * df.loc[x.index, \"size\"]).sum()),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Compute contract VWAP\n",
    "    contract_agg[\"contract_vwap\"] = (\n",
    "        contract_agg[\"contract_price_sum_product\"] / \n",
    "        (contract_agg[\"contract_total_volume\"] + eps)\n",
    "    )\n",
    "    contract_agg.drop(columns=[\"contract_price_sum_product\"], inplace=True)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Join aggregates back to trades\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    df = df.merge(contract_agg, on=\"ticker\", how=\"left\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute trade-level features relative to contract\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Price vs VWAP (urgency signal)\n",
    "    df[\"trade_price_vs_contract_vwap\"] = (\n",
    "        df[\"price\"] / (df[\"contract_vwap\"] + eps)\n",
    "    ) - 1\n",
    "    \n",
    "    # Trade size as percentage of contract total\n",
    "    df[\"trade_size_pct_of_contract\"] = safe_divide(\n",
    "        df[\"size\"], df[\"contract_total_volume\"]\n",
    "    )\n",
    "    \n",
    "    # Trade size rank within contract (1 = largest)\n",
    "    df[\"trade_size_rank_in_contract\"] = df.groupby(\"ticker\")[\"size\"].rank(\n",
    "        ascending=False, method=\"min\"\n",
    "    )\n",
    "    \n",
    "    # Is this the largest trade in the contract?\n",
    "    df[\"is_largest_in_contract\"] = (\n",
    "        df[\"size\"] == df[\"contract_max_size\"]\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Contract volume to open interest (turnover)\n",
    "    df[\"contract_volume_to_oi\"] = safe_divide(\n",
    "        df[\"contract_total_volume\"], df[\"contract_oi\"]\n",
    "    )\n",
    "    \n",
    "    # OI change (positive = new positions being opened)\n",
    "    df[\"contract_oi_change\"] = df[\"contract_oi\"] - df[\"contract_oi_yesterday\"]\n",
    "    df[\"contract_oi_change_pct\"] = safe_divide(\n",
    "        df[\"contract_oi_change\"], df[\"contract_oi_yesterday\"]\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Contract-level concentration (HHI)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Compute HHI per contract (are a few trades dominating?)\n",
    "    contract_hhi = df.groupby(\"ticker\").apply(\n",
    "        lambda g: compute_hhi(g[\"size\"])\n",
    "    ).rename(\"contract_hhi\")\n",
    "    \n",
    "    df = df.merge(contract_hhi, on=\"ticker\", how=\"left\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE GROUP 2: CHAIN-LEVEL AGGREGATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_chain_level_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute chain-level (entire underlying for the day) aggregations.\n",
    "    \"\"\"\n",
    "    logger.debug(\"  Computing chain-level features...\")\n",
    "    \n",
    "    eps = CONFIG[\"epsilon\"]\n",
    "    otm_threshold = CONFIG[\"otm_threshold\"]\n",
    "    deep_otm_threshold = CONFIG[\"deep_otm_threshold\"]\n",
    "    short_dte_threshold = CONFIG[\"short_dte_threshold\"]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Basic chain aggregates\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    chain_total_volume = df[\"size\"].sum()\n",
    "    chain_total_notional = df[\"opt_trade_notional_value\"].sum()\n",
    "    chain_trade_count = len(df)\n",
    "    \n",
    "    df[\"chain_total_volume\"] = chain_total_volume\n",
    "    df[\"chain_total_notional\"] = chain_total_notional\n",
    "    df[\"chain_trade_count\"] = chain_trade_count\n",
    "    \n",
    "    # Total OI (sum of unique contracts)\n",
    "    chain_oi = df.groupby(\"ticker\")[\"open_interest_now\"].first().sum()\n",
    "    df[\"chain_total_oi\"] = chain_oi\n",
    "    df[\"chain_volume_to_oi\"] = safe_divide(chain_total_volume, chain_oi)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Contract share of chain\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    df[\"contract_volume_share_of_chain\"] = safe_divide(\n",
    "        df[\"contract_total_volume\"], chain_total_volume\n",
    "    )\n",
    "    df[\"contract_notional_share_of_chain\"] = safe_divide(\n",
    "        df[\"contract_total_notional\"], chain_total_notional\n",
    "    )\n",
    "    \n",
    "    # Trade share of chain\n",
    "    df[\"trade_size_pct_of_chain\"] = safe_divide(df[\"size\"], chain_total_volume)\n",
    "    df[\"trade_notional_pct_of_chain\"] = safe_divide(\n",
    "        df[\"opt_trade_notional_value\"], chain_total_notional\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Call/Put distribution\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    call_mask = df[\"option_type_call\"] == 1\n",
    "    put_mask = df[\"option_type_call\"] == 0\n",
    "    \n",
    "    chain_call_volume = df.loc[call_mask, \"size\"].sum()\n",
    "    chain_put_volume = df.loc[put_mask, \"size\"].sum()\n",
    "    chain_call_notional = df.loc[call_mask, \"opt_trade_notional_value\"].sum()\n",
    "    chain_put_notional = df.loc[put_mask, \"opt_trade_notional_value\"].sum()\n",
    "    \n",
    "    df[\"chain_call_volume\"] = chain_call_volume\n",
    "    df[\"chain_put_volume\"] = chain_put_volume\n",
    "    df[\"chain_call_notional\"] = chain_call_notional\n",
    "    df[\"chain_put_notional\"] = chain_put_notional\n",
    "    \n",
    "    df[\"call_put_volume_ratio\"] = safe_divide(chain_call_volume, chain_put_volume + 1)\n",
    "    df[\"call_put_notional_ratio\"] = safe_divide(chain_call_notional, chain_put_notional + 1)\n",
    "    df[\"call_volume_share\"] = safe_divide(chain_call_volume, chain_total_volume)\n",
    "    df[\"put_volume_share\"] = safe_divide(chain_put_volume, chain_total_volume)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Moneyness distribution\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    otm_mask = df[\"otm_percentage\"] > otm_threshold\n",
    "    deep_otm_mask = df[\"otm_percentage\"] > deep_otm_threshold\n",
    "    itm_mask = df[\"otm_percentage\"] < 0\n",
    "    atm_mask = (df[\"otm_percentage\"] >= -otm_threshold) & \\\n",
    "               (df[\"otm_percentage\"] <= otm_threshold)\n",
    "    \n",
    "    chain_otm_volume = df.loc[otm_mask, \"size\"].sum()\n",
    "    chain_deep_otm_volume = df.loc[deep_otm_mask, \"size\"].sum()\n",
    "    chain_itm_volume = df.loc[itm_mask, \"size\"].sum()\n",
    "    chain_atm_volume = df.loc[atm_mask, \"size\"].sum()\n",
    "    \n",
    "    df[\"chain_otm_volume\"] = chain_otm_volume\n",
    "    df[\"chain_deep_otm_volume\"] = chain_deep_otm_volume\n",
    "    df[\"chain_itm_volume\"] = chain_itm_volume\n",
    "    df[\"chain_atm_volume\"] = chain_atm_volume\n",
    "    \n",
    "    df[\"otm_volume_share\"] = safe_divide(chain_otm_volume, chain_total_volume)\n",
    "    df[\"deep_otm_volume_share\"] = safe_divide(chain_deep_otm_volume, chain_total_volume)\n",
    "    df[\"itm_volume_share\"] = safe_divide(chain_itm_volume, chain_total_volume)\n",
    "    df[\"atm_volume_share\"] = safe_divide(chain_atm_volume, chain_total_volume)\n",
    "    \n",
    "    # Volume-weighted average OTM\n",
    "    df[\"chain_volume_weighted_otm\"] = safe_divide(\n",
    "        (df[\"otm_percentage\"] * df[\"size\"]).sum(), chain_total_volume\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # DTE distribution\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    short_dte_mask = df[\"days_to_expiry\"] <= short_dte_threshold\n",
    "    long_dte_mask = df[\"days_to_expiry\"] > CONFIG[\"medium_dte_threshold\"]\n",
    "    \n",
    "    chain_short_dte_volume = df.loc[short_dte_mask, \"size\"].sum()\n",
    "    chain_long_dte_volume = df.loc[long_dte_mask, \"size\"].sum()\n",
    "    \n",
    "    df[\"chain_short_dte_volume\"] = chain_short_dte_volume\n",
    "    df[\"chain_long_dte_volume\"] = chain_long_dte_volume\n",
    "    \n",
    "    df[\"short_dte_volume_share\"] = safe_divide(chain_short_dte_volume, chain_total_volume)\n",
    "    df[\"long_dte_volume_share\"] = safe_divide(chain_long_dte_volume, chain_total_volume)\n",
    "    \n",
    "    # Volume-weighted average DTE\n",
    "    df[\"chain_volume_weighted_dte\"] = safe_divide(\n",
    "        (df[\"days_to_expiry\"] * df[\"size\"]).sum(), chain_total_volume\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Execution type distribution\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if \"cndn_multilegstrategy\" in df.columns:\n",
    "        multileg_volume = df.loc[df[\"cndn_multilegstrategy\"] == 1, \"size\"].sum()\n",
    "        df[\"multileg_volume_share\"] = safe_divide(multileg_volume, chain_total_volume)\n",
    "    else:\n",
    "        df[\"multileg_volume_share\"] = 0.0\n",
    "    \n",
    "    if \"cndn_autoelectronic\" in df.columns:\n",
    "        sweep_volume = df.loc[df[\"cndn_autoelectronic\"] == 1, \"size\"].sum()\n",
    "        df[\"sweep_volume_share\"] = safe_divide(sweep_volume, chain_total_volume)\n",
    "    else:\n",
    "        df[\"sweep_volume_share\"] = 0.0\n",
    "    \n",
    "    if \"cndn_floorexecuted\" in df.columns:\n",
    "        floor_volume = df.loc[df[\"cndn_floorexecuted\"] == 1, \"size\"].sum()\n",
    "        df[\"floor_volume_share\"] = safe_divide(floor_volume, chain_total_volume)\n",
    "    else:\n",
    "        df[\"floor_volume_share\"] = 0.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Chain-level concentration metrics\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Max contract share\n",
    "    contract_volumes = df.groupby(\"ticker\")[\"size\"].sum()\n",
    "    max_contract_share = safe_divide(contract_volumes.max(), chain_total_volume)\n",
    "    df[\"chain_max_contract_share\"] = max_contract_share\n",
    "    \n",
    "    # Contract HHI (concentration across contracts)\n",
    "    df[\"chain_contract_hhi\"] = compute_hhi(contract_volumes)\n",
    "    \n",
    "    # Number of unique contracts traded\n",
    "    df[\"chain_unique_contracts\"] = len(contract_volumes)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Greeks aggregation (chain-wide)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if \"delta_size\" in df.columns:\n",
    "        df[\"chain_net_delta\"] = df[\"delta_size\"].sum()\n",
    "        df[\"chain_net_gamma\"] = df[\"gamma_size\"].sum()\n",
    "        df[\"chain_net_vega\"] = df[\"vega_size\"].sum()\n",
    "        df[\"chain_net_theta\"] = df[\"theta_size\"].sum()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE GROUP 3: EXPIRY-LEVEL AGGREGATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_expiry_level_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute expiry-level (same expiration date, all strikes) aggregations.\n",
    "    \"\"\"\n",
    "    logger.debug(\"  Computing expiry-level features...\")\n",
    "    \n",
    "    eps = CONFIG[\"epsilon\"]\n",
    "    chain_total_volume = df[\"chain_total_volume\"].iloc[0]\n",
    "    chain_total_notional = df[\"chain_total_notional\"].iloc[0]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Aggregate by expiry\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    expiry_agg = df.groupby(\"opt_expiration_date\").agg(\n",
    "        expiry_total_volume=(\"size\", \"sum\"),\n",
    "        expiry_total_notional=(\"opt_trade_notional_value\", \"sum\"),\n",
    "        expiry_trade_count=(\"size\", \"count\"),\n",
    "        expiry_call_volume=(\"size\", lambda x: x[df.loc[x.index, \"option_type_call\"] == 1].sum()),\n",
    "        expiry_put_volume=(\"size\", lambda x: x[df.loc[x.index, \"option_type_call\"] == 0].sum()),\n",
    "        expiry_unique_strikes=(\"strike_price\", \"nunique\"),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Expiry share of chain\n",
    "    expiry_agg[\"expiry_volume_share_of_chain\"] = safe_divide(\n",
    "        expiry_agg[\"expiry_total_volume\"], chain_total_volume\n",
    "    )\n",
    "    expiry_agg[\"expiry_notional_share_of_chain\"] = safe_divide(\n",
    "        expiry_agg[\"expiry_total_notional\"], chain_total_notional\n",
    "    )\n",
    "    \n",
    "    # Call/put ratio within expiry\n",
    "    expiry_agg[\"expiry_call_put_ratio\"] = safe_divide(\n",
    "        expiry_agg[\"expiry_call_volume\"], expiry_agg[\"expiry_put_volume\"] + 1\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Join back to trades\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    df = df.merge(\n",
    "        expiry_agg[[\"opt_expiration_date\", \"expiry_total_volume\", \"expiry_total_notional\",\n",
    "                    \"expiry_trade_count\", \"expiry_volume_share_of_chain\",\n",
    "                    \"expiry_notional_share_of_chain\", \"expiry_call_put_ratio\",\n",
    "                    \"expiry_unique_strikes\"]],\n",
    "        on=\"opt_expiration_date\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Contract share within expiry\n",
    "    df[\"contract_volume_share_of_expiry\"] = safe_divide(\n",
    "        df[\"contract_total_volume\"], df[\"expiry_total_volume\"]\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Strike concentration within expiry (HHI)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    expiry_strike_hhi = df.groupby(\"opt_expiration_date\").apply(\n",
    "        lambda g: compute_hhi(g.groupby(\"strike_price\")[\"size\"].sum())\n",
    "    ).rename(\"expiry_strike_hhi\")\n",
    "    \n",
    "    df = df.merge(expiry_strike_hhi, on=\"opt_expiration_date\", how=\"left\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # OTM distribution within expiry\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    otm_threshold = CONFIG[\"otm_threshold\"]\n",
    "    \n",
    "    expiry_otm = df.groupby(\"opt_expiration_date\").apply(\n",
    "        lambda g: safe_divide(\n",
    "            g.loc[g[\"otm_percentage\"] > otm_threshold, \"size\"].sum(),\n",
    "            g[\"size\"].sum()\n",
    "        )\n",
    "    ).rename(\"expiry_otm_volume_share\")\n",
    "    \n",
    "    df = df.merge(expiry_otm, on=\"opt_expiration_date\", how=\"left\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE GROUP 4: HISTORICAL BASELINE COMPARISONS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_baseline_comparison_features(\n",
    "    df: pd.DataFrame, \n",
    "    baseline_df: pd.DataFrame,\n",
    "    underlying: str,\n",
    "    trade_date: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare today's activity to historical baselines.\n",
    "    \"\"\"\n",
    "    logger.debug(\"  Computing baseline comparison features...\")\n",
    "    \n",
    "    eps = CONFIG[\"epsilon\"]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Get baseline for this underlying and date\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    baseline_row = baseline_df[\n",
    "        (baseline_df[\"underlying\"] == underlying) &\n",
    "        (baseline_df[\"trade_date\"] == trade_date)\n",
    "    ]\n",
    "    \n",
    "    if len(baseline_row) == 0:\n",
    "        logger.warning(f\"    No baseline found for {underlying} on {trade_date}\")\n",
    "        # Fill with NaN for baseline features\n",
    "        baseline_features = [\n",
    "            \"chain_volume_vs_baseline_mean\", \"chain_volume_zscore\",\n",
    "            \"chain_notional_vs_baseline_mean\", \"chain_notional_zscore\",\n",
    "            \"avg_trade_size_vs_baseline\", \"avg_trade_size_zscore\",\n",
    "            \"call_share_vs_baseline\", \"call_share_zscore\",\n",
    "            \"otm_share_vs_baseline\", \"otm_share_zscore\",\n",
    "            \"deep_otm_share_vs_baseline\", \"deep_otm_share_zscore\",\n",
    "            \"short_dte_share_vs_baseline\", \"short_dte_share_zscore\",\n",
    "            \"max_contract_share_vs_baseline\", \"max_contract_share_zscore\",\n",
    "            \"volume_to_oi_vs_baseline\", \"volume_to_oi_zscore\",\n",
    "            \"vwap_premium_vs_baseline\", \"baseline_days_count\",\n",
    "            \"baseline_is_sufficient\"\n",
    "        ]\n",
    "        for feat in baseline_features:\n",
    "            df[feat] = np.nan\n",
    "        df[\"baseline_days_count\"] = 0\n",
    "        df[\"baseline_is_sufficient\"] = 0\n",
    "        return df\n",
    "    \n",
    "    baseline = baseline_row.iloc[0]\n",
    "    \n",
    "    # Store baseline metadata\n",
    "    df[\"baseline_days_count\"] = baseline[\"baseline_days_count\"]\n",
    "    df[\"baseline_is_sufficient\"] = baseline[\"baseline_is_sufficient\"]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Volume comparisons\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    chain_volume = df[\"chain_total_volume\"].iloc[0]\n",
    "    chain_notional = df[\"chain_total_notional\"].iloc[0]\n",
    "    \n",
    "    df[\"chain_volume_vs_baseline_mean\"] = safe_divide(\n",
    "        chain_volume, baseline[\"baseline_chain_volume_mean\"]\n",
    "    )\n",
    "    df[\"chain_volume_zscore\"] = compute_zscore(\n",
    "        chain_volume,\n",
    "        baseline[\"baseline_chain_volume_mean\"],\n",
    "        baseline[\"baseline_chain_volume_std\"]\n",
    "    )\n",
    "    \n",
    "    df[\"chain_notional_vs_baseline_mean\"] = safe_divide(\n",
    "        chain_notional, baseline[\"baseline_chain_notional_mean\"]\n",
    "    )\n",
    "    df[\"chain_notional_zscore\"] = compute_zscore(\n",
    "        chain_notional,\n",
    "        baseline[\"baseline_chain_notional_mean\"],\n",
    "        baseline[\"baseline_chain_notional_std\"]\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Trade size comparisons\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    avg_trade_size = df[\"size\"].mean()\n",
    "    \n",
    "    df[\"avg_trade_size_vs_baseline\"] = safe_divide(\n",
    "        avg_trade_size, baseline[\"baseline_avg_trade_size_mean\"]\n",
    "    )\n",
    "    df[\"avg_trade_size_zscore\"] = compute_zscore(\n",
    "        avg_trade_size,\n",
    "        baseline[\"baseline_avg_trade_size_mean\"],\n",
    "        baseline[\"baseline_avg_trade_size_std\"]\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Distribution comparisons\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    call_share = df[\"call_volume_share\"].iloc[0]\n",
    "    otm_share = df[\"otm_volume_share\"].iloc[0]\n",
    "    deep_otm_share = df[\"deep_otm_volume_share\"].iloc[0]\n",
    "    short_dte_share = df[\"short_dte_volume_share\"].iloc[0]\n",
    "    max_contract_share = df[\"chain_max_contract_share\"].iloc[0]\n",
    "    volume_to_oi = df[\"chain_volume_to_oi\"].iloc[0]\n",
    "    \n",
    "    # Call share\n",
    "    df[\"call_share_vs_baseline\"] = safe_divide(\n",
    "        call_share, baseline[\"baseline_call_volume_share_mean\"]\n",
    "    )\n",
    "    df[\"call_share_zscore\"] = compute_zscore(\n",
    "        call_share,\n",
    "        baseline[\"baseline_call_volume_share_mean\"],\n",
    "        baseline[\"baseline_call_volume_share_std\"]\n",
    "    )\n",
    "    \n",
    "    # OTM share\n",
    "    df[\"otm_share_vs_baseline\"] = safe_divide(\n",
    "        otm_share, baseline[\"baseline_otm_volume_share_mean\"]\n",
    "    )\n",
    "    df[\"otm_share_zscore\"] = compute_zscore(\n",
    "        otm_share,\n",
    "        baseline[\"baseline_otm_volume_share_mean\"],\n",
    "        baseline[\"baseline_otm_volume_share_std\"]\n",
    "    )\n",
    "    \n",
    "    # Deep OTM share\n",
    "    df[\"deep_otm_share_vs_baseline\"] = safe_divide(\n",
    "        deep_otm_share, baseline[\"baseline_deep_otm_volume_share_mean\"]\n",
    "    )\n",
    "    df[\"deep_otm_share_zscore\"] = compute_zscore(\n",
    "        deep_otm_share,\n",
    "        baseline[\"baseline_deep_otm_volume_share_mean\"],\n",
    "        baseline[\"baseline_deep_otm_volume_share_std\"]\n",
    "    )\n",
    "    \n",
    "    # Short DTE share\n",
    "    df[\"short_dte_share_vs_baseline\"] = safe_divide(\n",
    "        short_dte_share, baseline[\"baseline_short_dte_volume_share_mean\"]\n",
    "    )\n",
    "    df[\"short_dte_share_zscore\"] = compute_zscore(\n",
    "        short_dte_share,\n",
    "        baseline[\"baseline_short_dte_volume_share_mean\"],\n",
    "        baseline[\"baseline_short_dte_volume_share_std\"]\n",
    "    )\n",
    "    \n",
    "    # Max contract share (concentration)\n",
    "    df[\"max_contract_share_vs_baseline\"] = safe_divide(\n",
    "        max_contract_share, baseline[\"baseline_max_contract_share_mean\"]\n",
    "    )\n",
    "    df[\"max_contract_share_zscore\"] = compute_zscore(\n",
    "        max_contract_share,\n",
    "        baseline[\"baseline_max_contract_share_mean\"],\n",
    "        baseline[\"baseline_max_contract_share_std\"]\n",
    "    )\n",
    "    \n",
    "    # Volume to OI\n",
    "    df[\"volume_to_oi_vs_baseline\"] = safe_divide(\n",
    "        volume_to_oi, baseline[\"baseline_volume_to_oi_mean\"]\n",
    "    )\n",
    "    df[\"volume_to_oi_zscore\"] = compute_zscore(\n",
    "        volume_to_oi,\n",
    "        baseline[\"baseline_volume_to_oi_mean\"],\n",
    "        baseline[\"baseline_volume_to_oi_std\"]\n",
    "    )\n",
    "    \n",
    "    # VWAP premium\n",
    "    avg_vwap_premium = df[\"trade_price_vs_contract_vwap\"].mean()\n",
    "    df[\"vwap_premium_vs_baseline\"] = safe_divide(\n",
    "        avg_vwap_premium, baseline[\"baseline_vwap_premium_mean\"] + eps\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE GROUP 5: COMPOSITE SCORES (TYPE A DETECTION)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_composite_scores(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute composite scores optimized for Type A detection.\n",
    "    \"\"\"\n",
    "    logger.debug(\"  Computing composite scores...\")\n",
    "    \n",
    "    eps = CONFIG[\"epsilon\"]\n",
    "    zscore_clip = CONFIG[\"zscore_clip_max\"]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Time of day features\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Parse time from sip_timestamp if it's a string\n",
    "    if df[\"sip_timestamp\"].dtype == object:\n",
    "        try:\n",
    "            df[\"trade_datetime\"] = pd.to_datetime(df[\"sip_timestamp\"])\n",
    "            df[\"time_of_day_minutes\"] = (\n",
    "                df[\"trade_datetime\"].dt.hour * 60 + \n",
    "                df[\"trade_datetime\"].dt.minute - \n",
    "                9 * 60 - 30  # Minutes since 9:30 AM\n",
    "            )\n",
    "            # Clip to regular trading hours (0-390 minutes)\n",
    "            df[\"time_of_day_minutes\"] = df[\"time_of_day_minutes\"].clip(0, 390)\n",
    "        except Exception:\n",
    "            df[\"time_of_day_minutes\"] = 195  # Default to mid-day\n",
    "    else:\n",
    "        df[\"time_of_day_minutes\"] = 195  # Default to mid-day\n",
    "    \n",
    "    # Early session indicator (first 30 minutes)\n",
    "    df[\"is_early_session\"] = (df[\"time_of_day_minutes\"] <= 30).astype(int)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # URGENCY SCORE\n",
    "    # Captures time pressure signals\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Time component (earlier = higher urgency)\n",
    "    time_score = 1 - (df[\"time_of_day_minutes\"] / 390)\n",
    "    \n",
    "    # Price vs VWAP component (paying above average = urgency)\n",
    "    vwap_premium_norm = normalize_series(\n",
    "        df[\"trade_price_vs_contract_vwap\"].clip(-0.1, 0.2)\n",
    "    )\n",
    "    \n",
    "    # Sweep indicator (aggressive execution)\n",
    "    sweep_indicator = df.get(\"cndn_autoelectronic\", 0)\n",
    "    \n",
    "    df[\"urgency_score\"] = (\n",
    "        0.40 * vwap_premium_norm +\n",
    "        0.35 * time_score +\n",
    "        0.25 * sweep_indicator\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SIZE DOMINANCE SCORE\n",
    "    # Captures whether this trade is unusually large\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Trade size relative to contract\n",
    "    contract_pct_norm = normalize_series(\n",
    "        df[\"trade_size_pct_of_contract\"].clip(0, 1)\n",
    "    )\n",
    "    \n",
    "    # Trade size relative to chain\n",
    "    chain_pct_norm = normalize_series(\n",
    "        df[\"trade_size_pct_of_chain\"].clip(0, 0.1)  # Clip outliers\n",
    "    )\n",
    "    \n",
    "    # Inverse rank (rank 1 = highest score)\n",
    "    rank_score = 1 / (df[\"trade_size_rank_in_contract\"] + eps)\n",
    "    rank_score_norm = normalize_series(rank_score)\n",
    "    \n",
    "    # Contract turnover (volume / OI)\n",
    "    turnover_norm = normalize_series(\n",
    "        df[\"contract_volume_to_oi\"].clip(0, 5)\n",
    "    )\n",
    "    \n",
    "    df[\"size_dominance_score\"] = (\n",
    "        0.30 * contract_pct_norm +\n",
    "        0.30 * chain_pct_norm +\n",
    "        0.20 * rank_score_norm +\n",
    "        0.20 * turnover_norm\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # INFORMED FLOW SCORE (Primary Type A Detector)\n",
    "    # Combines key signals for informed directional betting\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # OTM component (higher OTM = more speculative/informed)\n",
    "    otm_norm = normalize_series(df[\"otm_percentage\"].clip(0, 50))\n",
    "    \n",
    "    # Near-dated component (lower DTE = catalyst-driven)\n",
    "    dte_score = 1 / (df[\"days_to_expiry\"].clip(1, 365) + eps)\n",
    "    dte_norm = normalize_series(dte_score)\n",
    "    \n",
    "    # Volume zscore component (unusual day for this underlying)\n",
    "    volume_zscore_clipped = clip_zscore(df[\"chain_volume_zscore\"]).fillna(0)\n",
    "    volume_zscore_norm = (volume_zscore_clipped + zscore_clip) / (2 * zscore_clip)\n",
    "    \n",
    "    # Contract concentration (bet concentrated in this contract)\n",
    "    concentration_norm = normalize_series(\n",
    "        df[\"contract_volume_share_of_chain\"].clip(0, 0.5)\n",
    "    )\n",
    "    \n",
    "    df[\"informed_flow_score\"] = (\n",
    "        0.25 * normalize_series(df[\"size_dominance_score\"]) +\n",
    "        0.20 * normalize_series(df[\"urgency_score\"]) +\n",
    "        0.20 * otm_norm +\n",
    "        0.15 * dte_norm +\n",
    "        0.10 * volume_zscore_norm +\n",
    "        0.10 * concentration_norm\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # BASELINE ANOMALY SCORE\n",
    "    # Purely statistical measure of how unusual this day is\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Clip and normalize z-scores\n",
    "    vol_z = clip_zscore(df[\"chain_volume_zscore\"]).fillna(0)\n",
    "    deep_otm_z = clip_zscore(df[\"deep_otm_share_zscore\"]).fillna(0)\n",
    "    max_contract_z = clip_zscore(df[\"max_contract_share_zscore\"]).fillna(0)\n",
    "    vol_oi_z = clip_zscore(df[\"volume_to_oi_zscore\"]).fillna(0)\n",
    "    \n",
    "    # Directional alignment: call share z-score for calls, negative for puts\n",
    "    call_z = clip_zscore(df[\"call_share_zscore\"]).fillna(0)\n",
    "    directional_z = np.where(\n",
    "        df[\"option_type_call\"] == 1,\n",
    "        call_z,  # For calls, positive call share is aligned\n",
    "        -call_z  # For puts, negative call share is aligned\n",
    "    )\n",
    "    \n",
    "    # Normalize all to 0-1 range (assuming z-scores are clipped to ±5)\n",
    "    df[\"baseline_anomaly_score\"] = (\n",
    "        0.30 * (vol_z + zscore_clip) / (2 * zscore_clip) +\n",
    "        0.20 * (deep_otm_z + zscore_clip) / (2 * zscore_clip) +\n",
    "        0.20 * (max_contract_z + zscore_clip) / (2 * zscore_clip) +\n",
    "        0.15 * (vol_oi_z + zscore_clip) / (2 * zscore_clip) +\n",
    "        0.15 * (directional_z + zscore_clip) / (2 * zscore_clip)\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # COMBINED TYPE A SCORE\n",
    "    # Final score optimized for Type A detection\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    df[\"type_a_score\"] = (\n",
    "        0.35 * normalize_series(df[\"informed_flow_score\"]) +\n",
    "        0.30 * normalize_series(df[\"size_dominance_score\"]) +\n",
    "        0.20 * normalize_series(df[\"baseline_anomaly_score\"]) +\n",
    "        0.15 * normalize_series(df[\"urgency_score\"])\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE GROUP 6: FALSE POSITIVE FILTERS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_false_positive_filters(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute features to filter out likely false positives.\n",
    "    \"\"\"\n",
    "    logger.debug(\"  Computing false positive filters...\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Trade type filters\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Multi-leg trades (usually hedges, not directional bets)\n",
    "    df[\"is_likely_multileg\"] = df.get(\"cndn_multilegstrategy\", 0)\n",
    "    \n",
    "    # Extended hours (different dynamics)\n",
    "    df[\"is_extended_hours\"] = df.get(\"condn_extendedhours\", 0)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Position type filters\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Likely closing position (OI decreased)\n",
    "    df[\"is_likely_closing\"] = (\n",
    "        df[\"contract_oi_change\"] < 0\n",
    "    ).astype(int)\n",
    "    \n",
    "    # ITM trades (often hedging or assignment-related)\n",
    "    df[\"is_itm_trade\"] = (df[\"otm_percentage\"] < 0).astype(int)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Liquidity filters\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Low liquidity contract\n",
    "    df[\"is_low_liquidity_contract\"] = (\n",
    "        df[\"contract_trade_count\"] < CONFIG[\"min_contract_trades\"]\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Low liquidity chain\n",
    "    df[\"is_low_liquidity_chain\"] = (\n",
    "        (df[\"chain_trade_count\"] < CONFIG[\"min_chain_trades\"]) |\n",
    "        (df[\"chain_total_volume\"] < CONFIG[\"min_chain_volume\"])\n",
    "    ).astype(int)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Data quality filters\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Insufficient baseline\n",
    "    df[\"is_insufficient_baseline\"] = (\n",
    "        df[\"baseline_is_sufficient\"] == 0\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Wide spread (illiquid, price less meaningful)\n",
    "    if \"close_ask\" in df.columns and \"close_bid\" in df.columns:\n",
    "        mid_price = (df[\"close_ask\"] + df[\"close_bid\"]) / 2\n",
    "        spread = df[\"close_ask\"] - df[\"close_bid\"]\n",
    "        df[\"relative_spread\"] = safe_divide(spread, mid_price)\n",
    "        df[\"is_wide_spread\"] = (df[\"relative_spread\"] > 0.10).astype(int)\n",
    "    else:\n",
    "        df[\"relative_spread\"] = np.nan\n",
    "        df[\"is_wide_spread\"] = 0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Combined False Positive Risk Score\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    df[\"false_positive_risk\"] = (\n",
    "        0.25 * df[\"is_likely_multileg\"] +\n",
    "        0.20 * df[\"is_likely_closing\"] +\n",
    "        0.15 * df[\"is_low_liquidity_contract\"] +\n",
    "        0.15 * df[\"is_low_liquidity_chain\"] +\n",
    "        0.10 * df[\"is_itm_trade\"] +\n",
    "        0.08 * df[\"is_extended_hours\"] +\n",
    "        0.05 * df[\"is_insufficient_baseline\"] +\n",
    "        0.02 * df[\"is_wide_spread\"]\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Final filter flag\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # High-quality signal flag (passes all major filters)\n",
    "    df[\"is_high_quality_signal\"] = (\n",
    "        (df[\"is_likely_multileg\"] == 0) &\n",
    "        (df[\"is_low_liquidity_chain\"] == 0) &\n",
    "        (df[\"is_extended_hours\"] == 0) &\n",
    "        (df[\"false_positive_risk\"] < 0.4)\n",
    "    ).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def process_single_file(\n",
    "    filepath: Path,\n",
    "    underlying: str,\n",
    "    trade_date: str,\n",
    "    baseline_folder: Path,\n",
    "    output_folder: Path\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Process a single file through all aggregation stages.\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load data\n",
    "        df = pd.read_parquet(filepath)\n",
    "        \n",
    "        # Filter to correct underlying (safety check)\n",
    "        if \"underlying\" in df.columns:\n",
    "            df = df[df[\"underlying\"] == underlying].copy()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            logger.warning(f\"  No trades found for {underlying} on {trade_date}\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"  Loaded {len(df):,} trades\")\n",
    "        \n",
    "        # Load baseline for this ticker\n",
    "        baseline_file = baseline_folder / f\"baseline_{underlying}_daily.parquet\"\n",
    "        if baseline_file.exists():\n",
    "            baseline_df = pd.read_parquet(baseline_file)\n",
    "            logger.debug(f\"  Loaded baseline: {len(baseline_df)} rows\")\n",
    "        else:\n",
    "            logger.warning(f\"  No baseline file found for {underlying}\")\n",
    "            baseline_df = pd.DataFrame()  # Empty dataframe, will trigger NaN baseline features\n",
    "        \n",
    "        # Store original row count\n",
    "        original_count = len(df)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # APPLY FEATURE ENGINEERING STAGES\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Stage 1: Contract-level features\n",
    "        df = compute_contract_level_features(df)\n",
    "        \n",
    "        # Stage 2: Chain-level features\n",
    "        df = compute_chain_level_features(df)\n",
    "        \n",
    "        # Stage 3: Expiry-level features\n",
    "        df = compute_expiry_level_features(df)\n",
    "        \n",
    "        # Stage 4: Historical baseline comparisons\n",
    "        df = compute_baseline_comparison_features(\n",
    "            df, baseline_df, underlying, trade_date\n",
    "        )\n",
    "        \n",
    "        # Stage 5: Composite scores\n",
    "        df = compute_composite_scores(df)\n",
    "        \n",
    "        # Stage 6: False positive filters\n",
    "        df = compute_false_positive_filters(df)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # SAVE OUTPUT\n",
    "        # =====================================================================\n",
    "        \n",
    "        output_filename = f\"{underlying}_aggregatedfeatures_{trade_date}.parquet\"\n",
    "        output_path = output_folder / output_filename\n",
    "        \n",
    "        df.to_parquet(output_path, index=False)\n",
    "        \n",
    "        # Log summary statistics\n",
    "        high_quality_count = df[\"is_high_quality_signal\"].sum()\n",
    "        avg_type_a_score = df[\"type_a_score\"].mean()\n",
    "        max_type_a_score = df[\"type_a_score\"].max()\n",
    "        \n",
    "        logger.info(f\"  Saved: {output_filename}\")\n",
    "        logger.info(f\"    Rows: {len(df):,} | Columns: {len(df.columns)}\")\n",
    "        logger.info(f\"    High-quality signals: {high_quality_count:,} \"\n",
    "                   f\"({100*high_quality_count/len(df):.1f}%)\")\n",
    "        logger.info(f\"    Type A Score: mean={avg_type_a_score:.3f}, \"\n",
    "                   f\"max={max_type_a_score:.3f}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"  Error processing {filepath.name}: {e}\")\n",
    "        import traceback\n",
    "        logger.debug(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(\"PHASE 03B-2: AGGREGATED FEATURE ENGINEERING\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    input_folder = CONFIG[\"input_folder\"]\n",
    "    baseline_folder = CONFIG[\"baseline_folder\"]\n",
    "    output_folder = CONFIG[\"output_folder\"]\n",
    "    \n",
    "    # Create output folder if needed\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    logger.info(f\"Input folder: {input_folder}\")\n",
    "    logger.info(f\"Baseline folder: {baseline_folder}\")\n",
    "    logger.info(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # VERIFY BASELINE FOLDER EXISTS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(\"CHECKING BASELINE FOLDER\")\n",
    "    logger.info(\"-\" * 70)\n",
    "    \n",
    "    if not baseline_folder.exists():\n",
    "        logger.error(f\"Baseline folder not found: {baseline_folder}\")\n",
    "        logger.error(\"Please run Phase 03B-1 first to generate baselines.\")\n",
    "        return\n",
    "    \n",
    "    # Count available baseline files\n",
    "    baseline_files = list(baseline_folder.glob(\"baseline_*_daily.parquet\"))\n",
    "    logger.info(f\"Found {len(baseline_files)} baseline files in folder\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SCAN INPUT FILES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(\"SCANNING INPUT FILES\")\n",
    "    logger.info(\"-\" * 70)\n",
    "    \n",
    "    # Parse date filters\n",
    "    start_date = CONFIG[\"start_date\"]\n",
    "    end_date = CONFIG[\"end_date\"]\n",
    "    tickers_to_process = CONFIG[\"tickers_to_process\"]\n",
    "    \n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d') if start_date else None\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d') if end_date else None\n",
    "    \n",
    "    files_to_process = []\n",
    "    \n",
    "    for filepath in input_folder.glob(\"*_perrowfeatures_*.parquet\"):\n",
    "        ticker, date_str = parse_filename(filepath.name)\n",
    "        \n",
    "        if ticker is None:\n",
    "            continue\n",
    "        \n",
    "        # Apply ticker filter\n",
    "        if tickers_to_process and ticker not in tickers_to_process:\n",
    "            continue\n",
    "        \n",
    "        # Apply date filter\n",
    "        file_dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        if start_dt and file_dt < start_dt:\n",
    "            continue\n",
    "        if end_dt and file_dt > end_dt:\n",
    "            continue\n",
    "        \n",
    "        files_to_process.append((ticker, date_str, filepath))\n",
    "    \n",
    "    # Sort by date then ticker\n",
    "    files_to_process = sorted(files_to_process, key=lambda x: (x[1], x[0]))\n",
    "    \n",
    "    logger.info(f\"Found {len(files_to_process)} files to process\")\n",
    "    \n",
    "    if not files_to_process:\n",
    "        logger.warning(\"No files to process!\")\n",
    "        return\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PROCESS FILES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(\"PROCESSING FILES\")\n",
    "    logger.info(\"-\" * 70)\n",
    "    \n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for i, (ticker, date_str, filepath) in enumerate(files_to_process):\n",
    "        logger.info(f\"\\n[{i+1}/{len(files_to_process)}] {ticker} - {date_str}\")\n",
    "        \n",
    "        success = process_single_file(\n",
    "            filepath=filepath,\n",
    "            underlying=ticker,\n",
    "            trade_date=date_str,\n",
    "            baseline_folder=baseline_folder,\n",
    "            output_folder=output_folder\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            success_count += 1\n",
    "        else:\n",
    "            error_count += 1\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SUMMARY\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\" * 70)\n",
    "    logger.info(\"PROCESSING COMPLETE\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(f\"  Successful: {success_count}\")\n",
    "    logger.info(f\"  Errors: {error_count}\")\n",
    "    logger.info(f\"  Total: {len(files_to_process)}\")\n",
    "    \n",
    "    if success_count > 0:\n",
    "        # Count output files\n",
    "        output_files = list(output_folder.glob(\"*_aggregatedfeatures_*.parquet\"))\n",
    "        logger.info(f\"\\nOutput files created: {len(output_files)}\")\n",
    "        \n",
    "        # Sample column count from first file\n",
    "        if output_files:\n",
    "            sample_df = pd.read_parquet(output_files[0])\n",
    "            logger.info(f\"Output columns per file: {len(sample_df.columns)}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
