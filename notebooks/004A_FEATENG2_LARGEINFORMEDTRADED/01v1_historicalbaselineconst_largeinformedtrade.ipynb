{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================================================================\n",
    "PHASE 04A-1: HISTORICAL BASELINE CONSTRUCTION FOR TYPE A DETECTION [LARGE INFORMED TRADE]\n",
    "============================================================================================\n",
    "\n",
    "Pipeline Position: \n",
    "    Phase 03 (Per-Row Features) → **Phase 04A-1** → Phase 04A-2 (Aggregated Features)\n",
    "\n",
    "Purpose:\n",
    "    Builds rolling historical baseline statistics per underlying ticker. These\n",
    "    baselines are essential for determining what is \"unusual\" - a 10,000 contract\n",
    "    day might be quiet for AAPL but 50x normal for a small biotech.\n",
    "\n",
    "Input:\n",
    "    - Phase 04A-1 output files: {TICKER}_perrowfeatures_YYYY-MM-DD.parquet\n",
    "    \n",
    "Output:\n",
    "    - Baseline lookup table: baseline_underlying_daily.parquet\n",
    "    - One row per underlying per trade_date with rolling 20-day statistics\n",
    "\n",
    "Key Features:\n",
    "    - Strict no-lookahead: baseline for date T uses only data from T-20 to T-1\n",
    "    - Handles missing days gracefully (weekends, holidays, no trading activity)\n",
    "    - Flags underlyings with insufficient history (baseline_days_count < 5)\n",
    "    - Computes statistics for volume, notional, trade size, directional bias,\n",
    "      moneyness distribution, DTE distribution, and concentration metrics\n",
    "\n",
    "Author: Mariceline Querubin\n",
    "Created: 2026-02-08\n",
    "Version: 1.0\n",
    "\n",
    "Dependencies:\n",
    "    - pandas >= 1.5.0\n",
    "    - numpy >= 1.20.0\n",
    "    - pyarrow >= 10.0.0 (for parquet I/O)\n",
    "\n",
    "Usage:\n",
    "    python \"scriptname.py\"\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Input folder containing Phase 03A output files\n",
    "    \"input_folder\": Path(r\"D:\\cyclelabs_codes\\CL_20251120_siphontrades\\01_FIXINGRAWDATA\\output_2_perrowfeateng\"),\n",
    "    \n",
    "    # Output folder for baseline table\n",
    "    \"output_folder\": Path(r\"D:\\cyclelabs_codes\\CL_20251120_siphontrades\\01_FIXINGRAWDATA\\output_4a1_baseline\"),\n",
    "    \n",
    "    # Lookback window for baseline calculation (trading days)\n",
    "    \"lookback_days\": 20,\n",
    "    \n",
    "    # Minimum days required for valid baseline\n",
    "    \"min_baseline_days\": 5,\n",
    "    \n",
    "    # Date range to process (inclusive)\n",
    "    # Set to None to process all available dates\n",
    "    \"start_date\": \"2024-02-01\",\n",
    "    \"end_date\": \"2025-12-31\",\n",
    "    \n",
    "    # Tickers to process (None or [] for all tickers)\n",
    "    \"tickers_to_process\": None,  # e.g., [\"AAPL\", \"TSLA\", \"CIFR\"]\n",
    "    \n",
    "    # OTM percentage thresholds\n",
    "    \"otm_threshold\": 5.0,        # > 5% = OTM\n",
    "    \"deep_otm_threshold\": 20.0,  # > 20% = Deep OTM\n",
    "    \n",
    "    # DTE thresholds\n",
    "    \"short_dte_threshold\": 14,   # <= 14 days = short-dated\n",
    "    \"medium_dte_threshold\": 45,  # <= 45 days = medium-dated\n",
    "    \n",
    "    # Logging level\n",
    "    \"log_level\": logging.INFO,\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# LOGGING SETUP\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=CONFIG[\"log_level\"],\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def parse_filename(filename: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Parse ticker and date from Phase 03A output filename.\n",
    "    \n",
    "    Expected format: {TICKER}_perrowfeatures_YYYY-MM-DD.parquet\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of the parquet file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (ticker, date_str) or (None, None) if parsing fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove extension\n",
    "        base = filename.replace('.parquet', '')\n",
    "        \n",
    "        # Split by '_perrowfeatures_'\n",
    "        parts = base.split('_perrowfeatures_')\n",
    "        if len(parts) != 2:\n",
    "            return None, None\n",
    "            \n",
    "        ticker = parts[0]\n",
    "        date_str = parts[1]\n",
    "        \n",
    "        # Validate date format\n",
    "        datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        \n",
    "        return ticker, date_str\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def get_trading_days_before(target_date: str, n_days: int, \n",
    "                            available_dates: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get the N trading days before target_date from available dates.\n",
    "    \n",
    "    Args:\n",
    "        target_date: Target date in YYYY-MM-DD format\n",
    "        n_days: Number of trading days to look back\n",
    "        available_dates: List of available trading dates\n",
    "        \n",
    "    Returns:\n",
    "        List of trading dates (sorted ascending, oldest first)\n",
    "    \"\"\"\n",
    "    target_dt = datetime.strptime(target_date, '%Y-%m-%d')\n",
    "    \n",
    "    # Filter dates strictly before target\n",
    "    prior_dates = [d for d in available_dates \n",
    "                   if datetime.strptime(d, '%Y-%m-%d') < target_dt]\n",
    "    \n",
    "    # Sort descending and take n_days\n",
    "    prior_dates_sorted = sorted(prior_dates, reverse=True)[:n_days]\n",
    "    \n",
    "    # Return in ascending order (oldest first)\n",
    "    return sorted(prior_dates_sorted)\n",
    "\n",
    "\n",
    "def safe_divide(numerator: float, denominator: float, \n",
    "                default: float = 0.0) -> float:\n",
    "    \"\"\"Safe division handling zero denominators.\"\"\"\n",
    "    if denominator == 0 or pd.isna(denominator):\n",
    "        return default\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def compute_hhi(values: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Compute Herfindahl-Hirschman Index for concentration measurement.\n",
    "    \n",
    "    HHI = sum((share_i)^2) where share_i = value_i / total\n",
    "    Range: 1/n (perfectly distributed) to 1.0 (single entity dominates)\n",
    "    \"\"\"\n",
    "    total = values.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    shares = values / total\n",
    "    return (shares ** 2).sum()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DAILY STATISTICS COMPUTATION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_daily_statistics(df: pd.DataFrame, trade_date: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute aggregate statistics for a single underlying on a single day.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing all trades for one underlying on one day\n",
    "            Expected to have Phase 03A columns\n",
    "        trade_date: The date being processed (for reference)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of daily statistics\n",
    "    \"\"\"\n",
    "    stats = {\"trade_date\": trade_date}\n",
    "    \n",
    "    # Handle empty dataframe\n",
    "    if len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # VOLUME & NOTIONAL STATISTICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    stats[\"daily_chain_volume\"] = df[\"size\"].sum()\n",
    "    stats[\"daily_chain_notional\"] = df[\"opt_trade_notional_value\"].sum()\n",
    "    stats[\"daily_chain_trade_count\"] = len(df)\n",
    "    stats[\"daily_avg_trade_size\"] = df[\"size\"].mean()\n",
    "    stats[\"daily_median_trade_size\"] = df[\"size\"].median()\n",
    "    stats[\"daily_max_trade_size\"] = df[\"size\"].max()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # CALL/PUT DISTRIBUTION\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    call_mask = df[\"option_type_call\"] == 1\n",
    "    put_mask = df[\"option_type_call\"] == 0\n",
    "    \n",
    "    call_volume = df.loc[call_mask, \"size\"].sum()\n",
    "    put_volume = df.loc[put_mask, \"size\"].sum()\n",
    "    call_notional = df.loc[call_mask, \"opt_trade_notional_value\"].sum()\n",
    "    put_notional = df.loc[put_mask, \"opt_trade_notional_value\"].sum()\n",
    "    \n",
    "    stats[\"daily_call_volume\"] = call_volume\n",
    "    stats[\"daily_put_volume\"] = put_volume\n",
    "    stats[\"daily_call_volume_share\"] = safe_divide(\n",
    "        call_volume, stats[\"daily_chain_volume\"]\n",
    "    )\n",
    "    stats[\"daily_call_put_volume_ratio\"] = safe_divide(\n",
    "        call_volume, put_volume + 1  # Add 1 to avoid division by zero\n",
    "    )\n",
    "    stats[\"daily_call_put_notional_ratio\"] = safe_divide(\n",
    "        call_notional, put_notional + 1\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # MONEYNESS DISTRIBUTION\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # OTM: otm_percentage > threshold (positive = OTM for both calls and puts)\n",
    "    otm_threshold = CONFIG[\"otm_threshold\"]\n",
    "    deep_otm_threshold = CONFIG[\"deep_otm_threshold\"]\n",
    "    \n",
    "    otm_mask = df[\"otm_percentage\"] > otm_threshold\n",
    "    deep_otm_mask = df[\"otm_percentage\"] > deep_otm_threshold\n",
    "    itm_mask = df[\"otm_percentage\"] < 0\n",
    "    atm_mask = (df[\"otm_percentage\"] >= -otm_threshold) & \\\n",
    "               (df[\"otm_percentage\"] <= otm_threshold)\n",
    "    \n",
    "    otm_volume = df.loc[otm_mask, \"size\"].sum()\n",
    "    deep_otm_volume = df.loc[deep_otm_mask, \"size\"].sum()\n",
    "    itm_volume = df.loc[itm_mask, \"size\"].sum()\n",
    "    atm_volume = df.loc[atm_mask, \"size\"].sum()\n",
    "    \n",
    "    stats[\"daily_otm_volume\"] = otm_volume\n",
    "    stats[\"daily_deep_otm_volume\"] = deep_otm_volume\n",
    "    stats[\"daily_itm_volume\"] = itm_volume\n",
    "    stats[\"daily_atm_volume\"] = atm_volume\n",
    "    \n",
    "    stats[\"daily_otm_volume_share\"] = safe_divide(\n",
    "        otm_volume, stats[\"daily_chain_volume\"]\n",
    "    )\n",
    "    stats[\"daily_deep_otm_volume_share\"] = safe_divide(\n",
    "        deep_otm_volume, stats[\"daily_chain_volume\"]\n",
    "    )\n",
    "    stats[\"daily_itm_volume_share\"] = safe_divide(\n",
    "        itm_volume, stats[\"daily_chain_volume\"]\n",
    "    )\n",
    "    \n",
    "    # Volume-weighted average OTM percentage\n",
    "    if stats[\"daily_chain_volume\"] > 0:\n",
    "        stats[\"daily_volume_weighted_otm_pct\"] = (\n",
    "            (df[\"otm_percentage\"] * df[\"size\"]).sum() / stats[\"daily_chain_volume\"]\n",
    "        )\n",
    "    else:\n",
    "        stats[\"daily_volume_weighted_otm_pct\"] = 0.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # DTE DISTRIBUTION\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    short_dte_threshold = CONFIG[\"short_dte_threshold\"]\n",
    "    medium_dte_threshold = CONFIG[\"medium_dte_threshold\"]\n",
    "    \n",
    "    short_dte_mask = df[\"days_to_expiry\"] <= short_dte_threshold\n",
    "    medium_dte_mask = (df[\"days_to_expiry\"] > short_dte_threshold) & \\\n",
    "                      (df[\"days_to_expiry\"] <= medium_dte_threshold)\n",
    "    long_dte_mask = df[\"days_to_expiry\"] > medium_dte_threshold\n",
    "    \n",
    "    short_dte_volume = df.loc[short_dte_mask, \"size\"].sum()\n",
    "    medium_dte_volume = df.loc[medium_dte_mask, \"size\"].sum()\n",
    "    long_dte_volume = df.loc[long_dte_mask, \"size\"].sum()\n",
    "    \n",
    "    stats[\"daily_short_dte_volume\"] = short_dte_volume\n",
    "    stats[\"daily_medium_dte_volume\"] = medium_dte_volume\n",
    "    stats[\"daily_long_dte_volume\"] = long_dte_volume\n",
    "    \n",
    "    stats[\"daily_short_dte_volume_share\"] = safe_divide(\n",
    "        short_dte_volume, stats[\"daily_chain_volume\"]\n",
    "    )\n",
    "    stats[\"daily_medium_dte_volume_share\"] = safe_divide(\n",
    "        medium_dte_volume, stats[\"daily_chain_volume\"]\n",
    "    )\n",
    "    stats[\"daily_long_dte_volume_share\"] = safe_divide(\n",
    "        long_dte_volume, stats[\"daily_chain_volume\"]\n",
    "    )\n",
    "    \n",
    "    # Volume-weighted average DTE\n",
    "    if stats[\"daily_chain_volume\"] > 0:\n",
    "        stats[\"daily_volume_weighted_dte\"] = (\n",
    "            (df[\"days_to_expiry\"] * df[\"size\"]).sum() / stats[\"daily_chain_volume\"]\n",
    "        )\n",
    "    else:\n",
    "        stats[\"daily_volume_weighted_dte\"] = 0.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # CONCENTRATION METRICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Contract-level concentration (what % does the most active contract have?)\n",
    "    contract_volumes = df.groupby(\"ticker\")[\"size\"].sum()\n",
    "    \n",
    "    if len(contract_volumes) > 0:\n",
    "        max_contract_volume = contract_volumes.max()\n",
    "        stats[\"daily_max_contract_share\"] = safe_divide(\n",
    "            max_contract_volume, stats[\"daily_chain_volume\"]\n",
    "        )\n",
    "        stats[\"daily_contract_hhi\"] = compute_hhi(contract_volumes)\n",
    "        stats[\"daily_unique_contracts\"] = len(contract_volumes)\n",
    "    else:\n",
    "        stats[\"daily_max_contract_share\"] = 0.0\n",
    "        stats[\"daily_contract_hhi\"] = 0.0\n",
    "        stats[\"daily_unique_contracts\"] = 0\n",
    "    \n",
    "    # Trade-level HHI within the chain\n",
    "    stats[\"daily_trade_hhi\"] = compute_hhi(df[\"size\"])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PRICE / URGENCY METRICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Compute VWAP premium per contract, then average across chain\n",
    "    # This requires groupby contract first\n",
    "    if \"price\" in df.columns and len(df) > 0:\n",
    "        # For each contract, compute VWAP\n",
    "        contract_vwap = df.groupby(\"ticker\").apply(\n",
    "            lambda g: (g[\"price\"] * g[\"size\"]).sum() / g[\"size\"].sum()\n",
    "            if g[\"size\"].sum() > 0 else np.nan\n",
    "        )\n",
    "        \n",
    "        # Join back and compute premium\n",
    "        df_temp = df.copy()\n",
    "        df_temp[\"contract_vwap\"] = df_temp[\"ticker\"].map(contract_vwap)\n",
    "        df_temp[\"price_vs_vwap\"] = (df_temp[\"price\"] / df_temp[\"contract_vwap\"]) - 1\n",
    "        \n",
    "        # Volume-weighted average VWAP premium\n",
    "        valid_mask = df_temp[\"price_vs_vwap\"].notna()\n",
    "        if valid_mask.sum() > 0:\n",
    "            stats[\"daily_avg_vwap_premium\"] = (\n",
    "                (df_temp.loc[valid_mask, \"price_vs_vwap\"] * \n",
    "                 df_temp.loc[valid_mask, \"size\"]).sum() / \n",
    "                df_temp.loc[valid_mask, \"size\"].sum()\n",
    "            )\n",
    "        else:\n",
    "            stats[\"daily_avg_vwap_premium\"] = 0.0\n",
    "    else:\n",
    "        stats[\"daily_avg_vwap_premium\"] = 0.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # OPEN INTEREST / TURNOVER METRICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Get unique contracts with their OI\n",
    "    if \"open_interest_now\" in df.columns:\n",
    "        contract_oi = df.groupby(\"ticker\")[\"open_interest_now\"].first()\n",
    "        total_oi = contract_oi.sum()\n",
    "        \n",
    "        stats[\"daily_total_oi\"] = total_oi\n",
    "        stats[\"daily_volume_to_oi\"] = safe_divide(\n",
    "            stats[\"daily_chain_volume\"], total_oi\n",
    "        )\n",
    "    else:\n",
    "        stats[\"daily_total_oi\"] = 0\n",
    "        stats[\"daily_volume_to_oi\"] = 0.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # EXECUTION TYPE DISTRIBUTION\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Multi-leg trades\n",
    "    if \"cndn_multilegstrategy\" in df.columns:\n",
    "        multileg_volume = df.loc[df[\"cndn_multilegstrategy\"] == 1, \"size\"].sum()\n",
    "        stats[\"daily_multileg_volume_share\"] = safe_divide(\n",
    "            multileg_volume, stats[\"daily_chain_volume\"]\n",
    "        )\n",
    "    else:\n",
    "        stats[\"daily_multileg_volume_share\"] = 0.0\n",
    "    \n",
    "    # Sweep/aggressive electronic\n",
    "    if \"cndn_autoelectronic\" in df.columns:\n",
    "        sweep_volume = df.loc[df[\"cndn_autoelectronic\"] == 1, \"size\"].sum()\n",
    "        stats[\"daily_sweep_volume_share\"] = safe_divide(\n",
    "            sweep_volume, stats[\"daily_chain_volume\"]\n",
    "        )\n",
    "    else:\n",
    "        stats[\"daily_sweep_volume_share\"] = 0.0\n",
    "    \n",
    "    # Floor trades\n",
    "    if \"cndn_floorexecuted\" in df.columns:\n",
    "        floor_volume = df.loc[df[\"cndn_floorexecuted\"] == 1, \"size\"].sum()\n",
    "        stats[\"daily_floor_volume_share\"] = safe_divide(\n",
    "            floor_volume, stats[\"daily_chain_volume\"]\n",
    "        )\n",
    "    else:\n",
    "        stats[\"daily_floor_volume_share\"] = 0.0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BASELINE COMPUTATION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_baseline_for_date(\n",
    "    underlying: str,\n",
    "    target_date: str,\n",
    "    historical_stats: pd.DataFrame,\n",
    "    lookback_days: int,\n",
    "    min_baseline_days: int\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Compute rolling baseline statistics for a single underlying on a single date.\n",
    "    \n",
    "    Uses only data from dates strictly prior to target_date to avoid lookahead.\n",
    "    \n",
    "    Args:\n",
    "        underlying: Ticker symbol\n",
    "        target_date: Date to compute baseline for\n",
    "        historical_stats: DataFrame of daily statistics (output of compute_daily_statistics)\n",
    "        lookback_days: Number of trading days to look back\n",
    "        min_baseline_days: Minimum days required for valid baseline\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of baseline statistics\n",
    "    \"\"\"\n",
    "    baseline = {\n",
    "        \"underlying\": underlying,\n",
    "        \"trade_date\": target_date,\n",
    "    }\n",
    "    \n",
    "    # Filter to dates strictly before target_date\n",
    "    target_dt = datetime.strptime(target_date, '%Y-%m-%d')\n",
    "    hist = historical_stats[\n",
    "        historical_stats[\"trade_date\"].apply(\n",
    "            lambda x: datetime.strptime(x, '%Y-%m-%d') < target_dt\n",
    "        )\n",
    "    ].copy()\n",
    "    \n",
    "    # Sort by date descending and take lookback_days\n",
    "    hist = hist.sort_values(\"trade_date\", ascending=False).head(lookback_days)\n",
    "    \n",
    "    # Record how many days we have\n",
    "    baseline[\"baseline_days_count\"] = len(hist)\n",
    "    baseline[\"baseline_is_sufficient\"] = int(len(hist) >= min_baseline_days)\n",
    "    \n",
    "    # If no historical data, return empty baseline with NaN values\n",
    "    if len(hist) == 0:\n",
    "        baseline_metrics = [\n",
    "            \"baseline_chain_volume_mean\", \"baseline_chain_volume_std\",\n",
    "            \"baseline_chain_volume_median\", \"baseline_chain_notional_mean\",\n",
    "            \"baseline_chain_notional_std\", \"baseline_chain_trade_count_mean\",\n",
    "            \"baseline_chain_trade_count_std\", \"baseline_avg_trade_size_mean\",\n",
    "            \"baseline_avg_trade_size_std\", \"baseline_call_volume_share_mean\",\n",
    "            \"baseline_call_volume_share_std\", \"baseline_call_put_ratio_mean\",\n",
    "            \"baseline_call_put_ratio_std\", \"baseline_otm_volume_share_mean\",\n",
    "            \"baseline_otm_volume_share_std\", \"baseline_deep_otm_volume_share_mean\",\n",
    "            \"baseline_deep_otm_volume_share_std\", \"baseline_short_dte_volume_share_mean\",\n",
    "            \"baseline_short_dte_volume_share_std\", \"baseline_max_contract_share_mean\",\n",
    "            \"baseline_max_contract_share_std\", \"baseline_volume_to_oi_mean\",\n",
    "            \"baseline_volume_to_oi_std\", \"baseline_vwap_premium_mean\",\n",
    "            \"baseline_vwap_premium_std\", \"baseline_multileg_share_mean\",\n",
    "            \"baseline_sweep_share_mean\"\n",
    "        ]\n",
    "        for metric in baseline_metrics:\n",
    "            baseline[metric] = np.nan\n",
    "        return baseline\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # COMPUTE ROLLING STATISTICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Volume statistics\n",
    "    baseline[\"baseline_chain_volume_mean\"] = hist[\"daily_chain_volume\"].mean()\n",
    "    baseline[\"baseline_chain_volume_std\"] = hist[\"daily_chain_volume\"].std()\n",
    "    baseline[\"baseline_chain_volume_median\"] = hist[\"daily_chain_volume\"].median()\n",
    "    baseline[\"baseline_chain_volume_p25\"] = hist[\"daily_chain_volume\"].quantile(0.25)\n",
    "    baseline[\"baseline_chain_volume_p75\"] = hist[\"daily_chain_volume\"].quantile(0.75)\n",
    "    \n",
    "    # Notional statistics\n",
    "    baseline[\"baseline_chain_notional_mean\"] = hist[\"daily_chain_notional\"].mean()\n",
    "    baseline[\"baseline_chain_notional_std\"] = hist[\"daily_chain_notional\"].std()\n",
    "    baseline[\"baseline_chain_notional_median\"] = hist[\"daily_chain_notional\"].median()\n",
    "    \n",
    "    # Trade count statistics\n",
    "    baseline[\"baseline_chain_trade_count_mean\"] = hist[\"daily_chain_trade_count\"].mean()\n",
    "    baseline[\"baseline_chain_trade_count_std\"] = hist[\"daily_chain_trade_count\"].std()\n",
    "    \n",
    "    # Trade size statistics\n",
    "    baseline[\"baseline_avg_trade_size_mean\"] = hist[\"daily_avg_trade_size\"].mean()\n",
    "    baseline[\"baseline_avg_trade_size_std\"] = hist[\"daily_avg_trade_size\"].std()\n",
    "    baseline[\"baseline_max_trade_size_mean\"] = hist[\"daily_max_trade_size\"].mean()\n",
    "    \n",
    "    # Call/Put distribution\n",
    "    baseline[\"baseline_call_volume_share_mean\"] = hist[\"daily_call_volume_share\"].mean()\n",
    "    baseline[\"baseline_call_volume_share_std\"] = hist[\"daily_call_volume_share\"].std()\n",
    "    baseline[\"baseline_call_put_ratio_mean\"] = hist[\"daily_call_put_volume_ratio\"].mean()\n",
    "    baseline[\"baseline_call_put_ratio_std\"] = hist[\"daily_call_put_volume_ratio\"].std()\n",
    "    \n",
    "    # Moneyness distribution\n",
    "    baseline[\"baseline_otm_volume_share_mean\"] = hist[\"daily_otm_volume_share\"].mean()\n",
    "    baseline[\"baseline_otm_volume_share_std\"] = hist[\"daily_otm_volume_share\"].std()\n",
    "    baseline[\"baseline_deep_otm_volume_share_mean\"] = hist[\"daily_deep_otm_volume_share\"].mean()\n",
    "    baseline[\"baseline_deep_otm_volume_share_std\"] = hist[\"daily_deep_otm_volume_share\"].std()\n",
    "    baseline[\"baseline_itm_volume_share_mean\"] = hist[\"daily_itm_volume_share\"].mean()\n",
    "    baseline[\"baseline_volume_weighted_otm_mean\"] = hist[\"daily_volume_weighted_otm_pct\"].mean()\n",
    "    \n",
    "    # DTE distribution\n",
    "    baseline[\"baseline_short_dte_volume_share_mean\"] = hist[\"daily_short_dte_volume_share\"].mean()\n",
    "    baseline[\"baseline_short_dte_volume_share_std\"] = hist[\"daily_short_dte_volume_share\"].std()\n",
    "    baseline[\"baseline_medium_dte_volume_share_mean\"] = hist[\"daily_medium_dte_volume_share\"].mean()\n",
    "    baseline[\"baseline_long_dte_volume_share_mean\"] = hist[\"daily_long_dte_volume_share\"].mean()\n",
    "    baseline[\"baseline_volume_weighted_dte_mean\"] = hist[\"daily_volume_weighted_dte\"].mean()\n",
    "    \n",
    "    # Concentration metrics\n",
    "    baseline[\"baseline_max_contract_share_mean\"] = hist[\"daily_max_contract_share\"].mean()\n",
    "    baseline[\"baseline_max_contract_share_std\"] = hist[\"daily_max_contract_share\"].std()\n",
    "    baseline[\"baseline_contract_hhi_mean\"] = hist[\"daily_contract_hhi\"].mean()\n",
    "    baseline[\"baseline_unique_contracts_mean\"] = hist[\"daily_unique_contracts\"].mean()\n",
    "    \n",
    "    # Turnover metrics\n",
    "    baseline[\"baseline_volume_to_oi_mean\"] = hist[\"daily_volume_to_oi\"].mean()\n",
    "    baseline[\"baseline_volume_to_oi_std\"] = hist[\"daily_volume_to_oi\"].std()\n",
    "    \n",
    "    # Price/Urgency metrics\n",
    "    baseline[\"baseline_vwap_premium_mean\"] = hist[\"daily_avg_vwap_premium\"].mean()\n",
    "    baseline[\"baseline_vwap_premium_std\"] = hist[\"daily_avg_vwap_premium\"].std()\n",
    "    \n",
    "    # Execution type metrics\n",
    "    baseline[\"baseline_multileg_share_mean\"] = hist[\"daily_multileg_volume_share\"].mean()\n",
    "    baseline[\"baseline_sweep_share_mean\"] = hist[\"daily_sweep_volume_share\"].mean()\n",
    "    baseline[\"baseline_floor_share_mean\"] = hist[\"daily_floor_volume_share\"].mean()\n",
    "    \n",
    "    return baseline\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def scan_input_files(input_folder: Path, \n",
    "                     tickers_to_process: Optional[List[str]] = None,\n",
    "                     start_date: Optional[str] = None,\n",
    "                     end_date: Optional[str] = None) -> Dict[str, List[Tuple[str, Path]]]:\n",
    "    \"\"\"\n",
    "    Scan input folder and organize files by underlying ticker.\n",
    "    \n",
    "    Args:\n",
    "        input_folder: Path to folder containing Phase 03A output files\n",
    "        tickers_to_process: List of tickers to include (None for all)\n",
    "        start_date: Start date filter (inclusive)\n",
    "        end_date: End date filter (inclusive)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping ticker -> list of (date, filepath) tuples, sorted by date\n",
    "    \"\"\"\n",
    "    files_by_ticker = {}\n",
    "    \n",
    "    # Parse date filters\n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d') if start_date else None\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d') if end_date else None\n",
    "    \n",
    "    # Scan all parquet files\n",
    "    for filepath in input_folder.glob(\"*_perrowfeatures_*.parquet\"):\n",
    "        ticker, date_str = parse_filename(filepath.name)\n",
    "        \n",
    "        if ticker is None:\n",
    "            continue\n",
    "            \n",
    "        # Apply ticker filter\n",
    "        if tickers_to_process and ticker not in tickers_to_process:\n",
    "            continue\n",
    "            \n",
    "        # Apply date filter\n",
    "        file_dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        if start_dt and file_dt < start_dt:\n",
    "            continue\n",
    "        if end_dt and file_dt > end_dt:\n",
    "            continue\n",
    "            \n",
    "        # Add to dictionary\n",
    "        if ticker not in files_by_ticker:\n",
    "            files_by_ticker[ticker] = []\n",
    "        files_by_ticker[ticker].append((date_str, filepath))\n",
    "    \n",
    "    # Sort each ticker's files by date\n",
    "    for ticker in files_by_ticker:\n",
    "        files_by_ticker[ticker] = sorted(files_by_ticker[ticker], key=lambda x: x[0])\n",
    "    \n",
    "    return files_by_ticker\n",
    "\n",
    "\n",
    "def process_ticker(ticker: str, \n",
    "                   file_list: List[Tuple[str, Path]],\n",
    "                   lookback_days: int,\n",
    "                   min_baseline_days: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a single ticker: compute daily stats, then rolling baselines.\n",
    "    \n",
    "    Args:\n",
    "        ticker: Underlying ticker symbol\n",
    "        file_list: List of (date, filepath) tuples for this ticker\n",
    "        lookback_days: Number of trading days for rolling window\n",
    "        min_baseline_days: Minimum days required for valid baseline\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame of baseline statistics for this ticker\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing {ticker}: {len(file_list)} files\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: Compute daily statistics for each date\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    daily_stats_list = []\n",
    "    \n",
    "    for date_str, filepath in file_list:\n",
    "        try:\n",
    "            df = pd.read_parquet(filepath)\n",
    "            \n",
    "            # Verify this is the correct ticker\n",
    "            if \"underlying\" in df.columns:\n",
    "                df = df[df[\"underlying\"] == ticker]\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                logger.debug(f\"  {date_str}: No trades for {ticker}\")\n",
    "                continue\n",
    "                \n",
    "            stats = compute_daily_statistics(df, date_str)\n",
    "            if stats:\n",
    "                daily_stats_list.append(stats)\n",
    "                logger.debug(f\"  {date_str}: {stats['daily_chain_volume']} contracts, \"\n",
    "                           f\"{stats['daily_chain_trade_count']} trades\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"  Error processing {filepath.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(daily_stats_list) == 0:\n",
    "        logger.warning(f\"No daily statistics computed for {ticker}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    daily_stats_df = pd.DataFrame(daily_stats_list)\n",
    "    logger.info(f\"  Computed daily stats for {len(daily_stats_df)} days\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: Compute rolling baselines for each date\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    baseline_list = []\n",
    "    all_dates = sorted(daily_stats_df[\"trade_date\"].unique())\n",
    "    \n",
    "    for target_date in all_dates:\n",
    "        baseline = compute_baseline_for_date(\n",
    "            underlying=ticker,\n",
    "            target_date=target_date,\n",
    "            historical_stats=daily_stats_df,\n",
    "            lookback_days=lookback_days,\n",
    "            min_baseline_days=min_baseline_days\n",
    "        )\n",
    "        baseline_list.append(baseline)\n",
    "    \n",
    "    baseline_df = pd.DataFrame(baseline_list)\n",
    "    logger.info(f\"  Computed baselines for {len(baseline_df)} days\")\n",
    "    \n",
    "    return baseline_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(\"PHASE 03B-1: HISTORICAL BASELINE CONSTRUCTION\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    input_folder = CONFIG[\"input_folder\"]\n",
    "    output_folder = CONFIG[\"output_folder\"]\n",
    "    \n",
    "    # Create output folder if needed\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    logger.info(f\"Input folder: {input_folder}\")\n",
    "    logger.info(f\"Output folder: {output_folder}\")\n",
    "    logger.info(f\"Lookback days: {CONFIG['lookback_days']}\")\n",
    "    logger.info(f\"Minimum baseline days: {CONFIG['min_baseline_days']}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SCAN INPUT FILES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(\"SCANNING INPUT FILES\")\n",
    "    logger.info(\"-\" * 70)\n",
    "    \n",
    "    files_by_ticker = scan_input_files(\n",
    "        input_folder=input_folder,\n",
    "        tickers_to_process=CONFIG[\"tickers_to_process\"],\n",
    "        start_date=CONFIG[\"start_date\"],\n",
    "        end_date=CONFIG[\"end_date\"]\n",
    "    )\n",
    "    \n",
    "    if not files_by_ticker:\n",
    "        logger.error(\"No input files found!\")\n",
    "        return\n",
    "    \n",
    "    total_files = sum(len(v) for v in files_by_ticker.values())\n",
    "    logger.info(f\"Found {len(files_by_ticker)} tickers, {total_files} total files\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PROCESS EACH TICKER\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(\"PROCESSING TICKERS\")\n",
    "    logger.info(\"-\" * 70)\n",
    "    \n",
    "    all_baselines = []\n",
    "    \n",
    "    for i, (ticker, file_list) in enumerate(sorted(files_by_ticker.items())):\n",
    "        logger.info(f\"\\n[{i+1}/{len(files_by_ticker)}] Processing {ticker}\")\n",
    "        \n",
    "        baseline_df = process_ticker(\n",
    "            ticker=ticker,\n",
    "            file_list=file_list,\n",
    "            lookback_days=CONFIG[\"lookback_days\"],\n",
    "            min_baseline_days=CONFIG[\"min_baseline_days\"]\n",
    "        )\n",
    "        \n",
    "        if len(baseline_df) > 0:\n",
    "            all_baselines.append(baseline_df)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE OUTPUT (Per-Ticker Files)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if not all_baselines:\n",
    "        logger.error(\"No baselines computed!\")\n",
    "        return\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(\"SAVING OUTPUT\")\n",
    "    logger.info(\"-\" * 70)\n",
    "    \n",
    "    total_rows = 0\n",
    "    total_files = 0\n",
    "    \n",
    "    for baseline_df in all_baselines:\n",
    "        if len(baseline_df) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get the underlying ticker from the dataframe\n",
    "        underlying_ticker = baseline_df[\"underlying\"].iloc[0]\n",
    "        \n",
    "        # Sort by date\n",
    "        baseline_df = baseline_df.sort_values(\"trade_date\").reset_index(drop=True)\n",
    "        \n",
    "        # Save to parquet with ticker in filename\n",
    "        output_filename = f\"baseline_{underlying_ticker}_daily.parquet\"\n",
    "        output_path = output_folder / output_filename\n",
    "        baseline_df.to_parquet(output_path, index=False)\n",
    "        \n",
    "        total_rows += len(baseline_df)\n",
    "        total_files += 1\n",
    "        \n",
    "        logger.info(f\"Saved: {output_filename} ({len(baseline_df)} rows)\")\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(f\"Total files saved: {total_files}\")\n",
    "    logger.info(f\"Total rows: {total_rows:,}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    logger.info(\"\\nBaseline Coverage Summary:\")\n",
    "    final_baseline_df = pd.concat(all_baselines, ignore_index=True)\n",
    "    coverage = final_baseline_df.groupby(\"underlying\").agg({\n",
    "        \"trade_date\": \"count\",\n",
    "        \"baseline_days_count\": \"mean\",\n",
    "        \"baseline_is_sufficient\": \"mean\"\n",
    "    }).rename(columns={\n",
    "        \"trade_date\": \"total_days\",\n",
    "        \"baseline_days_count\": \"avg_baseline_days\",\n",
    "        \"baseline_is_sufficient\": \"pct_sufficient\"\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"\\n{coverage.head(20).to_string()}\")\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\" * 70)\n",
    "    logger.info(\"PHASE 04A-1 Historical Baseline Construction for the Large Informed Trade -- COMPLETE\")\n",
    "    logger.info(\"=\" * 70)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
