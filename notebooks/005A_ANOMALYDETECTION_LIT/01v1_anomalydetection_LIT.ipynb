{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b67ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PHASE 04: ANOMALY DETECTION FOR TYPE A UNUSUAL OPTIONS ACTIVITY\n",
    "================================================================================\n",
    "\n",
    "Pipeline Position: \n",
    "    Phase 03B-2 (Aggregated Features) → **Phase 04** → Manual Review / Trading\n",
    "\n",
    "Purpose:\n",
    "    Detects Type A unusual options activity (large informed trades) using \n",
    "    per-underlying Isolation Forest models trained on rolling historical data.\n",
    "    \n",
    "Architecture:\n",
    "    - Per-underlying models: Each underlying has its own IF model\n",
    "    - Rolling training: 30-day lookback, minimum 20 days required\n",
    "    - Hierarchical detection: Chain filter → Trade scoring → Contract aggregation\n",
    "    - Percentile-based thresholds: Top 5% of trades per underlying flagged\n",
    "\n",
    "Input:\n",
    "    - Phase 03B-2 output files: {TICKER}_aggregatedfeatures_YYYY-MM-DD.parquet\n",
    "    \n",
    "Output:\n",
    "    - uoa_daily_summary_YYYY-MM-DD.csv: One row per underlying that passed filter\n",
    "    - uoa_flagged_contracts_YYYY-MM-DD.csv: Contracts with flagged trades\n",
    "    - uoa_flagged_trades_YYYY-MM-DD.parquet: All trades with anomaly scores\n",
    "    - uoa_anomalies_YYYY-MM-DD.csv: Flat ranked list of all anomalies\n",
    "    - uoa_run_log_YYYY-MM-DD.json: Run metadata\n",
    "\n",
    "Author: [Your Name]\n",
    "Created: 2026-02-XX\n",
    "Version: 1.0\n",
    "\n",
    "Dependencies:\n",
    "    - pandas >= 1.5.0\n",
    "    - numpy >= 1.20.0\n",
    "    - scikit-learn >= 1.0.0\n",
    "    - pyarrow >= 10.0.0\n",
    "\n",
    "Usage:\n",
    "    python Phase_04_anomaly_detection.py\n",
    "\n",
    "Important Notes: \n",
    "-input_folder = must contain one ticker data only from Phase 04A-2\n",
    "-output_folder = must be subdivided into per-ticker subfolders to avoid confusion\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Input folder containing Phase 03B-2 output files\n",
    "    \"input_folder\": Path(r\"D:\\cyclelabs_codes\\CL_20251120_siphontrades\\01_FIXINGRAWDATA\\output_4a2_aggfeateng\"),\n",
    "    \n",
    "    # Output folder for anomaly detection results\n",
    "    \"output_folder\": Path(r\"D:\\cyclelabs_codes\\CL_20251120_siphontrades\\01_FIXINGRAWDATA\\output_5a_anomaly\\CIFR\"),\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # DATE CONFIGURATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Option 1: Single target date\n",
    "    #   - Set \"single_date_mode\": True\n",
    "    #   - Set \"target_date\": \"YYYY-MM-DD\"\n",
    "    #\n",
    "    # Option 2: Multiple target dates (date range)\n",
    "    #   - Set \"single_date_mode\": False\n",
    "    #   - Set \"start_date\" and \"end_date\": \"YYYY-MM-DD\"\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    \"single_date_mode\": False,  # True = single date, False = date range\n",
    "    \n",
    "    # For single date mode\n",
    "    \"target_date\": \"2024-03-15\",\n",
    "    \n",
    "    # For multiple dates mode (ignored if single_date_mode is True)\n",
    "    \"start_date\": \"2025-01-01\",\n",
    "    \"end_date\": \"2025-12-31\",\n",
    "    \n",
    "    # Rolling window for training (trading days)\n",
    "    \"training_window_days\": 30,\n",
    "    \n",
    "    # Minimum history required for valid model\n",
    "    \"min_history_days\": 20,\n",
    "    \n",
    "    # Chain-level filter thresholds\n",
    "    \"chain_volume_zscore_threshold\": 2.0,\n",
    "    \"chain_min_volume\": 20,\n",
    "    \"chain_min_trades\": 2,\n",
    "    \n",
    "    # Anomaly threshold (percentile-based)\n",
    "    \"anomaly_percentile\": 90,  # Top 5% flagged\n",
    "    \n",
    "    # Contract aggregation\n",
    "    \"contract_top_k\": 5,  # Top-K mean for contract scores\n",
    "    \n",
    "    # Isolation Forest parameters\n",
    "    \"if_contamination\": 0.05,  # Conservative estimate\n",
    "    \"if_n_estimators\": 100,\n",
    "    \"if_random_state\": 42,\n",
    "    \n",
    "    # Features for Isolation Forest\n",
    "    \"if_features\": [\n",
    "        'trade_size_pct_of_chain',\n",
    "        'trade_size_pct_of_contract',\n",
    "        'contract_volume_share_of_chain',\n",
    "        'trade_price_vs_contract_vwap',\n",
    "        'otm_percentage',\n",
    "        'days_to_expiry',\n",
    "        'contract_volume_to_oi',\n",
    "        'contract_hhi',\n",
    "    ],\n",
    "    \n",
    "    # Logging level\n",
    "    \"log_level\": logging.INFO,\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# LOGGING SETUP\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=CONFIG[\"log_level\"],\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def parse_filename(filename: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Parse ticker and date from Phase 03B-2 output filename.\n",
    "    \n",
    "    Expected format: {TICKER}_aggregatedfeatures_YYYY-MM-DD.parquet\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base = filename.replace('.parquet', '')\n",
    "        parts = base.split('_aggregatedfeatures_')\n",
    "        if len(parts) != 2:\n",
    "            return None, None\n",
    "        ticker = parts[0]\n",
    "        date_str = parts[1]\n",
    "        datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        return ticker, date_str\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def get_available_dates_for_underlying(\n",
    "    input_folder: Path, \n",
    "    underlying: str\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get all available dates for an underlying, sorted ascending.\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    pattern = f\"{underlying}_aggregatedfeatures_*.parquet\"\n",
    "    for filepath in input_folder.glob(pattern):\n",
    "        _, date_str = parse_filename(filepath.name)\n",
    "        if date_str:\n",
    "            dates.append(date_str)\n",
    "    return sorted(dates)\n",
    "\n",
    "\n",
    "def get_trading_days_before(\n",
    "    target_date: str, \n",
    "    available_dates: List[str], \n",
    "    n_days: int\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get N trading days strictly before target_date.\n",
    "    \"\"\"\n",
    "    target_dt = datetime.strptime(target_date, '%Y-%m-%d')\n",
    "    prior_dates = [d for d in available_dates \n",
    "                   if datetime.strptime(d, '%Y-%m-%d') < target_dt]\n",
    "    prior_dates_sorted = sorted(prior_dates, reverse=True)[:n_days]\n",
    "    return sorted(prior_dates_sorted)\n",
    "\n",
    "\n",
    "def load_underlying_data(\n",
    "    input_folder: Path, \n",
    "    underlying: str, \n",
    "    date_str: str\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load Phase 03B-2 data for a specific underlying and date.\n",
    "    \"\"\"\n",
    "    filename = f\"{underlying}_aggregatedfeatures_{date_str}.parquet\"\n",
    "    filepath = input_folder / filename\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return pd.read_parquet(filepath)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CHAIN-LEVEL FILTER\n",
    "# =============================================================================\n",
    "\n",
    "def check_chain_filter(df: pd.DataFrame) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Check if underlying passes chain-level filter.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (passes: bool, reason: str)\n",
    "    \"\"\"\n",
    "    # Get chain-level stats (same for all rows in the df)\n",
    "    if len(df) == 0:\n",
    "        return False, \"no_trades\"\n",
    "    \n",
    "    chain_volume_zscore = df['chain_volume_zscore'].iloc[0]\n",
    "    chain_total_volume = df['chain_total_volume'].iloc[0]\n",
    "    chain_trade_count = df['chain_trade_count'].iloc[0]\n",
    "    baseline_is_sufficient = df['baseline_is_sufficient'].iloc[0]\n",
    "    \n",
    "    # Check each condition\n",
    "    if pd.isna(chain_volume_zscore):\n",
    "        return False, \"missing_zscore\"\n",
    "    \n",
    "    if baseline_is_sufficient != 1:\n",
    "        return False, \"insufficient_baseline\"\n",
    "    \n",
    "    if chain_volume_zscore < CONFIG[\"chain_volume_zscore_threshold\"]:\n",
    "        return False, f\"volume_zscore_{chain_volume_zscore:.2f}_below_threshold\"\n",
    "    \n",
    "    if chain_total_volume < CONFIG[\"chain_min_volume\"]:\n",
    "        return False, f\"volume_{chain_total_volume}_below_minimum\"\n",
    "    \n",
    "    if chain_trade_count < CONFIG[\"chain_min_trades\"]:\n",
    "        return False, f\"trade_count_{chain_trade_count}_below_minimum\"\n",
    "    \n",
    "    return True, \"passed\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ISOLATION FOREST TRAINING AND SCORING\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_features(df: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare feature matrix for Isolation Forest.\n",
    "    Handles missing values and infinite values.\n",
    "    \"\"\"\n",
    "    X = df[features].copy()\n",
    "    \n",
    "    # Replace inf with nan\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Fill nan with median of each column\n",
    "    for col in X.columns:\n",
    "        median_val = X[col].median()\n",
    "        if pd.isna(median_val):\n",
    "            median_val = 0\n",
    "        X[col] = X[col].fillna(median_val)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def train_isolation_forest(\n",
    "    training_data: pd.DataFrame,\n",
    "    features: List[str]\n",
    ") -> Optional[IsolationForest]:\n",
    "    \"\"\"\n",
    "    Train Isolation Forest on historical data.\n",
    "    \n",
    "    Args:\n",
    "        training_data: Historical trades (high-quality only)\n",
    "        features: List of feature column names\n",
    "        \n",
    "    Returns:\n",
    "        Trained IsolationForest model, or None if insufficient data\n",
    "    \"\"\"\n",
    "    if len(training_data) < 50:  # Minimum samples for meaningful model\n",
    "        return None\n",
    "    \n",
    "    X_train = prepare_features(training_data, features)\n",
    "    \n",
    "    model = IsolationForest(\n",
    "        contamination=CONFIG[\"if_contamination\"],\n",
    "        n_estimators=CONFIG[\"if_n_estimators\"],\n",
    "        random_state=CONFIG[\"if_random_state\"],\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def score_trades(\n",
    "    model: IsolationForest,\n",
    "    today_data: pd.DataFrame,\n",
    "    features: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Score today's trades using trained model.\n",
    "    \n",
    "    Returns dataframe with added anomaly_score column.\n",
    "    Higher score = more anomalous.\n",
    "    \"\"\"\n",
    "    df = today_data.copy()\n",
    "    \n",
    "    X = prepare_features(df, features)\n",
    "    \n",
    "    # score_samples returns negative values (more negative = more anomalous)\n",
    "    # We negate to get positive scores where higher = more anomalous\n",
    "    raw_scores = model.score_samples(X)\n",
    "    \n",
    "    # Normalize to 0-1 range (approximately)\n",
    "    # Typical raw scores range from -0.5 to 0.1\n",
    "    # We transform so that more anomalous = higher score\n",
    "    df['anomaly_score_raw'] = -raw_scores\n",
    "    \n",
    "    # Normalize to 0-1 using min-max within this underlying's scores\n",
    "    min_score = df['anomaly_score_raw'].min()\n",
    "    max_score = df['anomaly_score_raw'].max()\n",
    "    \n",
    "    if max_score > min_score:\n",
    "        df['anomaly_score'] = (df['anomaly_score_raw'] - min_score) / (max_score - min_score)\n",
    "    else:\n",
    "        df['anomaly_score'] = 0.5  # All same score\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def flag_anomalies_percentile(\n",
    "    df: pd.DataFrame,\n",
    "    percentile: float = 95\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flag trades above the percentile threshold.\n",
    "    \"\"\"\n",
    "    threshold = np.percentile(df['anomaly_score'], percentile)\n",
    "    df['is_anomaly'] = (df['anomaly_score'] >= threshold).astype(int)\n",
    "    df['anomaly_threshold'] = threshold\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONTRACT-LEVEL AGGREGATION\n",
    "# =============================================================================\n",
    "\n",
    "def aggregate_to_contract_level(df: pd.DataFrame, top_k: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate trade-level anomaly scores to contract level.\n",
    "    \n",
    "    Uses top-K mean: average of top K trade scores per contract.\n",
    "    \"\"\"\n",
    "    def top_k_mean(scores, k=top_k):\n",
    "        sorted_scores = sorted(scores, reverse=True)\n",
    "        top_scores = sorted_scores[:min(k, len(sorted_scores))]\n",
    "        return np.mean(top_scores) if top_scores else 0\n",
    "    \n",
    "    contract_agg = df.groupby('ticker').agg(\n",
    "        underlying=('underlying', 'first'),\n",
    "        contract_anomaly_score=('anomaly_score', lambda x: top_k_mean(x)),\n",
    "        contract_max_score=('anomaly_score', 'max'),\n",
    "        contract_mean_score=('anomaly_score', 'mean'),\n",
    "        num_trades=('size', 'count'),\n",
    "        num_flagged_trades=('is_anomaly', 'sum'),\n",
    "        total_volume=('size', 'sum'),\n",
    "        flagged_volume=('size', lambda x: x[df.loc[x.index, 'is_anomaly'] == 1].sum()),\n",
    "        total_notional=('opt_trade_notional_value', 'sum'),\n",
    "        flagged_notional=('opt_trade_notional_value', \n",
    "                          lambda x: x[df.loc[x.index, 'is_anomaly'] == 1].sum()),\n",
    "        contract_volume_share=('contract_volume_share_of_chain', 'first'),\n",
    "        otm_percentage=('otm_percentage', 'first'),\n",
    "        days_to_expiry=('days_to_expiry', 'first'),\n",
    "        option_type=('option_type_call', lambda x: 'CALL' if x.iloc[0] == 1 else 'PUT'),\n",
    "        strike_price=('strike_price', 'first'),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Sort by contract anomaly score descending\n",
    "    contract_agg = contract_agg.sort_values('contract_anomaly_score', ascending=False)\n",
    "    \n",
    "    return contract_agg\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FOR ONE UNDERLYING\n",
    "# =============================================================================\n",
    "\n",
    "def process_underlying(\n",
    "    underlying: str,\n",
    "    target_date: str,\n",
    "    input_folder: Path,\n",
    "    features: List[str]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Process a single underlying through the full anomaly detection pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results or status\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'underlying': underlying,\n",
    "        'target_date': target_date,\n",
    "        'status': None,\n",
    "        'reason': None,\n",
    "        'today_data': None,\n",
    "        'flagged_trades': None,\n",
    "        'contract_summary': None,\n",
    "        'chain_summary': None,\n",
    "    }\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 1: Load today's data\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    today_data = load_underlying_data(input_folder, underlying, target_date)\n",
    "    \n",
    "    if today_data is None or len(today_data) == 0:\n",
    "        result['status'] = 'skipped'\n",
    "        result['reason'] = 'no_data_for_target_date'\n",
    "        return result\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 2: Check chain-level filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    passes_filter, filter_reason = check_chain_filter(today_data)\n",
    "    \n",
    "    if not passes_filter:\n",
    "        result['status'] = 'filtered'\n",
    "        result['reason'] = filter_reason\n",
    "        return result\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 3: Get historical dates for training\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    available_dates = get_available_dates_for_underlying(input_folder, underlying)\n",
    "    historical_dates = get_trading_days_before(\n",
    "        target_date, \n",
    "        available_dates, \n",
    "        CONFIG[\"training_window_days\"]\n",
    "    )\n",
    "    \n",
    "    if len(historical_dates) < CONFIG[\"min_history_days\"]:\n",
    "        result['status'] = 'skipped'\n",
    "        result['reason'] = f'insufficient_history_{len(historical_dates)}_days'\n",
    "        return result\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 4: Load historical data for training\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    historical_dfs = []\n",
    "    for hist_date in historical_dates:\n",
    "        hist_df = load_underlying_data(input_folder, underlying, hist_date)\n",
    "        if hist_df is not None:\n",
    "            # Filter to high-quality signals only for training\n",
    "            hist_df_filtered = hist_df[hist_df['is_high_quality_signal'] == 1].copy()\n",
    "            if len(hist_df_filtered) > 0:\n",
    "                historical_dfs.append(hist_df_filtered)\n",
    "    \n",
    "    if not historical_dfs:\n",
    "        result['status'] = 'skipped'\n",
    "        result['reason'] = 'no_valid_historical_data'\n",
    "        return result\n",
    "    \n",
    "    training_data = pd.concat(historical_dfs, ignore_index=True)\n",
    "    \n",
    "    if len(training_data) < 50:\n",
    "        result['status'] = 'skipped'\n",
    "        result['reason'] = f'insufficient_training_samples_{len(training_data)}'\n",
    "        return result\n",
    "    \n",
    "    logger.debug(f\"  Training data: {len(training_data)} trades from {len(historical_dfs)} days\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 5: Train Isolation Forest\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    model = train_isolation_forest(training_data, features)\n",
    "    \n",
    "    if model is None:\n",
    "        result['status'] = 'skipped'\n",
    "        result['reason'] = 'model_training_failed'\n",
    "        return result\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 6: Score today's trades\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Filter today's data to high-quality signals for scoring\n",
    "    today_hq = today_data[today_data['is_high_quality_signal'] == 1].copy()\n",
    "    \n",
    "    if len(today_hq) == 0:\n",
    "        result['status'] = 'skipped'\n",
    "        result['reason'] = 'no_high_quality_trades_today'\n",
    "        return result\n",
    "    \n",
    "    scored_data = score_trades(model, today_hq, features)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 7: Flag anomalies (top 5%)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    flagged_data = flag_anomalies_percentile(\n",
    "        scored_data, \n",
    "        percentile=CONFIG[\"anomaly_percentile\"]\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 8: Aggregate to contract level\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    contract_summary = aggregate_to_contract_level(\n",
    "        flagged_data, \n",
    "        top_k=CONFIG[\"contract_top_k\"]\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 9: Build chain summary\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Helper function to convert numpy types to native Python\n",
    "    def to_native(val):\n",
    "        if isinstance(val, (np.integer,)):\n",
    "            return int(val)\n",
    "        elif isinstance(val, (np.floating,)):\n",
    "            return float(val)\n",
    "        elif pd.isna(val):\n",
    "            return None\n",
    "        return val\n",
    "    \n",
    "    chain_summary = {\n",
    "        'underlying': underlying,\n",
    "        'target_date': target_date,\n",
    "        'chain_volume_zscore': to_native(today_data['chain_volume_zscore'].iloc[0]),\n",
    "        'chain_total_volume': to_native(today_data['chain_total_volume'].iloc[0]),\n",
    "        'chain_total_notional': to_native(today_data['chain_total_notional'].iloc[0]),\n",
    "        'chain_trade_count': to_native(today_data['chain_trade_count'].iloc[0]),\n",
    "        'call_put_volume_ratio': to_native(today_data['call_put_volume_ratio'].iloc[0]),\n",
    "        'call_volume_share': to_native(today_data['call_volume_share'].iloc[0]),\n",
    "        'deep_otm_volume_share': to_native(today_data['deep_otm_volume_share'].iloc[0]),\n",
    "        'short_dte_volume_share': to_native(today_data['short_dte_volume_share'].iloc[0]),\n",
    "        'direction': 'BULLISH' if today_data['call_volume_share'].iloc[0] > 0.6 else \n",
    "                     ('BEARISH' if today_data['call_volume_share'].iloc[0] < 0.4 else 'NEUTRAL'),\n",
    "        'num_contracts_traded': len(contract_summary),\n",
    "        'num_contracts_flagged': len(contract_summary[contract_summary['num_flagged_trades'] > 0]),\n",
    "        'num_trades_scored': len(flagged_data),\n",
    "        'num_trades_flagged': int(flagged_data['is_anomaly'].sum()),\n",
    "        'flagged_volume': int(flagged_data.loc[flagged_data['is_anomaly'] == 1, 'size'].sum()),\n",
    "        'flagged_notional': float(flagged_data.loc[flagged_data['is_anomaly'] == 1, 'opt_trade_notional_value'].sum()),\n",
    "        'top_contract': contract_summary.iloc[0]['ticker'] if len(contract_summary) > 0 else None,\n",
    "        'top_contract_score': to_native(contract_summary.iloc[0]['contract_anomaly_score']) if len(contract_summary) > 0 else None,\n",
    "        'anomaly_threshold': to_native(flagged_data['anomaly_threshold'].iloc[0]),\n",
    "        'training_days': len(historical_dfs),\n",
    "        'training_samples': len(training_data),\n",
    "    }\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Return results\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    result['status'] = 'processed'\n",
    "    result['reason'] = 'success'\n",
    "    result['today_data'] = flagged_data\n",
    "    result['flagged_trades'] = flagged_data[flagged_data['is_anomaly'] == 1].copy()\n",
    "    result['contract_summary'] = contract_summary\n",
    "    result['chain_summary'] = chain_summary\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_anomaly_list(all_results: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate flat ranked list of all anomalies across all underlyings.\n",
    "    Includes ALL columns from the original trade data.\n",
    "    \"\"\"\n",
    "    anomaly_dfs = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result['status'] != 'processed':\n",
    "            continue\n",
    "        \n",
    "        flagged = result['flagged_trades']\n",
    "        if flagged is None or len(flagged) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Include all columns from flagged trades\n",
    "        anomaly_dfs.append(flagged.copy())\n",
    "    \n",
    "    if not anomaly_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Concatenate all flagged trades\n",
    "    anomaly_df = pd.concat(anomaly_dfs, ignore_index=True)\n",
    "    \n",
    "    # Sort by anomaly score descending\n",
    "    anomaly_df = anomaly_df.sort_values('anomaly_score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Add rank as first column\n",
    "    anomaly_df.insert(0, 'rank', range(1, len(anomaly_df) + 1))\n",
    "    \n",
    "    return anomaly_df\n",
    "\n",
    "\n",
    "def generate_daily_summary(all_results: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate daily summary with one row per underlying that passed filter.\n",
    "    \"\"\"\n",
    "    summary_rows = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result['status'] != 'processed':\n",
    "            continue\n",
    "        \n",
    "        chain = result['chain_summary']\n",
    "        summary_rows.append(chain)\n",
    "    \n",
    "    if not summary_rows:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    \n",
    "    # Sort by chain_volume_zscore descending (most unusual first)\n",
    "    summary_df = summary_df.sort_values('chain_volume_zscore', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Add rank\n",
    "    summary_df.insert(0, 'rank', range(1, len(summary_df) + 1))\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def generate_flagged_contracts(all_results: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate flagged contracts summary.\n",
    "    \"\"\"\n",
    "    contract_rows = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result['status'] != 'processed':\n",
    "            continue\n",
    "        \n",
    "        contracts = result['contract_summary']\n",
    "        if contracts is None:\n",
    "            continue\n",
    "        \n",
    "        # Only include contracts with flagged trades\n",
    "        flagged_contracts = contracts[contracts['num_flagged_trades'] > 0].copy()\n",
    "        \n",
    "        for _, contract in flagged_contracts.iterrows():\n",
    "            contract_rows.append({\n",
    "                'underlying': contract['underlying'],\n",
    "                'contract': contract['ticker'],\n",
    "                'option_type': contract['option_type'],\n",
    "                'strike_price': contract['strike_price'],\n",
    "                'days_to_expiry': contract['days_to_expiry'],\n",
    "                'otm_percentage': contract['otm_percentage'],\n",
    "                'contract_anomaly_score': contract['contract_anomaly_score'],\n",
    "                'contract_max_score': contract['contract_max_score'],\n",
    "                'contract_volume_share': contract['contract_volume_share'],\n",
    "                'num_trades': contract['num_trades'],\n",
    "                'num_flagged_trades': contract['num_flagged_trades'],\n",
    "                'total_volume': contract['total_volume'],\n",
    "                'flagged_volume': contract['flagged_volume'],\n",
    "                'total_notional': contract['total_notional'],\n",
    "                'flagged_notional': contract['flagged_notional'],\n",
    "            })\n",
    "    \n",
    "    if not contract_rows:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    contract_df = pd.DataFrame(contract_rows)\n",
    "    \n",
    "    # Sort by contract_anomaly_score descending\n",
    "    contract_df = contract_df.sort_values('contract_anomaly_score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Add rank\n",
    "    contract_df.insert(0, 'rank', range(1, len(contract_df) + 1))\n",
    "    \n",
    "    return contract_df\n",
    "\n",
    "\n",
    "def generate_all_flagged_trades(all_results: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine all scored trades from all underlyings.\n",
    "    \"\"\"\n",
    "    all_trades = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result['status'] != 'processed':\n",
    "            continue\n",
    "        \n",
    "        today_data = result['today_data']\n",
    "        if today_data is not None and len(today_data) > 0:\n",
    "            all_trades.append(today_data)\n",
    "    \n",
    "    if not all_trades:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    combined = pd.concat(all_trades, ignore_index=True)\n",
    "    \n",
    "    # Sort by anomaly_score descending\n",
    "    combined = combined.sort_values('anomaly_score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "def generate_run_log(\n",
    "    target_date: str,\n",
    "    all_results: List[Dict],\n",
    "    processing_time: float\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Generate run metadata log.\n",
    "    \"\"\"\n",
    "    status_counts = {}\n",
    "    for result in all_results:\n",
    "        status = result['status']\n",
    "        reason = result['reason']\n",
    "        key = f\"{status}_{reason}\"\n",
    "        status_counts[key] = status_counts.get(key, 0) + 1\n",
    "    \n",
    "    processed_results = [r for r in all_results if r['status'] == 'processed']\n",
    "    \n",
    "    total_flagged_trades = sum(\n",
    "        r['chain_summary']['num_trades_flagged'] \n",
    "        for r in processed_results\n",
    "    )\n",
    "    \n",
    "    total_flagged_volume = sum(\n",
    "        r['chain_summary']['flagged_volume'] \n",
    "        for r in processed_results\n",
    "    )\n",
    "    \n",
    "    total_flagged_notional = sum(\n",
    "        r['chain_summary']['flagged_notional'] \n",
    "        for r in processed_results\n",
    "    )\n",
    "    \n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    def convert_to_native(obj):\n",
    "        if isinstance(obj, (np.integer,)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating,)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_native(i) for i in obj]\n",
    "        return obj\n",
    "    \n",
    "    run_log = {\n",
    "        'run_timestamp': datetime.now().isoformat(),\n",
    "        'target_date': target_date,\n",
    "        'config': {\n",
    "            'training_window_days': CONFIG['training_window_days'],\n",
    "            'min_history_days': CONFIG['min_history_days'],\n",
    "            'chain_volume_zscore_threshold': CONFIG['chain_volume_zscore_threshold'],\n",
    "            'anomaly_percentile': CONFIG['anomaly_percentile'],\n",
    "            'if_features': CONFIG['if_features'],\n",
    "        },\n",
    "        'underlyings_total': len(all_results),\n",
    "        'underlyings_processed': len(processed_results),\n",
    "        'underlyings_filtered': len([r for r in all_results if r['status'] == 'filtered']),\n",
    "        'underlyings_skipped': len([r for r in all_results if r['status'] == 'skipped']),\n",
    "        'status_breakdown': status_counts,\n",
    "        'total_flagged_trades': int(total_flagged_trades),\n",
    "        'total_flagged_volume': int(total_flagged_volume),\n",
    "        'total_flagged_notional': float(total_flagged_notional),\n",
    "        'processing_time_seconds': round(processing_time, 2),\n",
    "    }\n",
    "    \n",
    "    return convert_to_native(run_log)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def get_all_underlyings_for_date(input_folder: Path, target_date: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get all underlyings that have data for the target date.\n",
    "    \"\"\"\n",
    "    underlyings = []\n",
    "    pattern = f\"*_aggregatedfeatures_{target_date}.parquet\"\n",
    "    \n",
    "    for filepath in input_folder.glob(pattern):\n",
    "        ticker, _ = parse_filename(filepath.name)\n",
    "        if ticker:\n",
    "            underlyings.append(ticker)\n",
    "    \n",
    "    return sorted(underlyings)\n",
    "\n",
    "\n",
    "def get_all_available_dates(input_folder: Path) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get all unique dates available in the input folder.\n",
    "    \"\"\"\n",
    "    dates = set()\n",
    "    \n",
    "    for filepath in input_folder.glob(\"*_aggregatedfeatures_*.parquet\"):\n",
    "        _, date_str = parse_filename(filepath.name)\n",
    "        if date_str:\n",
    "            dates.add(date_str)\n",
    "    \n",
    "    return sorted(dates)\n",
    "\n",
    "\n",
    "def get_dates_in_range(\n",
    "    input_folder: Path, \n",
    "    start_date: str, \n",
    "    end_date: str\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get all available dates within the specified range (inclusive).\n",
    "    \"\"\"\n",
    "    all_dates = get_all_available_dates(input_folder)\n",
    "    \n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    filtered_dates = [\n",
    "        d for d in all_dates\n",
    "        if start_dt <= datetime.strptime(d, '%Y-%m-%d') <= end_dt\n",
    "    ]\n",
    "    \n",
    "    return filtered_dates\n",
    "\n",
    "\n",
    "def process_single_date(target_date: str, input_folder: Path, output_folder: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a single target date through the full anomaly detection pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with run statistics\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(f\"PROCESSING DATE: {target_date}\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # GET ALL UNDERLYINGS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    underlyings = get_all_underlyings_for_date(input_folder, target_date)\n",
    "    \n",
    "    if not underlyings:\n",
    "        logger.warning(f\"No data found for date {target_date}\")\n",
    "        return {\n",
    "            'target_date': target_date,\n",
    "            'status': 'skipped',\n",
    "            'reason': 'no_data',\n",
    "            'underlyings_total': 0,\n",
    "        }\n",
    "    \n",
    "    logger.info(f\"Found {len(underlyings)} underlyings with data for {target_date}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PROCESS EACH UNDERLYING\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(\"PROCESSING UNDERLYINGS\")\n",
    "    logger.info(\"-\" * 70)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, underlying in enumerate(underlyings):\n",
    "        logger.info(f\"[{i+1}/{len(underlyings)}] Processing {underlying}...\")\n",
    "        \n",
    "        result = process_underlying(\n",
    "            underlying=underlying,\n",
    "            target_date=target_date,\n",
    "            input_folder=input_folder,\n",
    "            features=CONFIG[\"if_features\"]\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        if result['status'] == 'processed':\n",
    "            chain = result['chain_summary']\n",
    "            logger.info(f\"  ✓ Processed: {chain['num_trades_flagged']} flagged trades, \"\n",
    "                       f\"z-score={chain['chain_volume_zscore']:.2f}, \"\n",
    "                       f\"direction={chain['direction']}\")\n",
    "        else:\n",
    "            logger.info(f\"  ✗ {result['status'].upper()}: {result['reason']}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # GENERATE OUTPUTS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(\"GENERATING OUTPUTS\")\n",
    "    logger.info(\"-\" * 70)\n",
    "    \n",
    "    # Anomaly list (flat ranked list)\n",
    "    anomaly_df = generate_anomaly_list(all_results)\n",
    "    anomaly_path = output_folder / f\"uoa_anomalies_{target_date}.csv\"\n",
    "    if len(anomaly_df) > 0:\n",
    "        anomaly_df.to_csv(anomaly_path, index=False)\n",
    "        logger.info(f\"Saved anomaly list: {anomaly_path.name} ({len(anomaly_df)} anomalies)\")\n",
    "    else:\n",
    "        logger.info(\"No anomalies detected\")\n",
    "    \n",
    "    # Daily summary\n",
    "    summary_df = generate_daily_summary(all_results)\n",
    "    summary_path = output_folder / f\"uoa_daily_summary_{target_date}.csv\"\n",
    "    if len(summary_df) > 0:\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        logger.info(f\"Saved daily summary: {summary_path.name} ({len(summary_df)} underlyings)\")\n",
    "    \n",
    "    # Flagged contracts\n",
    "    contracts_df = generate_flagged_contracts(all_results)\n",
    "    contracts_path = output_folder / f\"uoa_flagged_contracts_{target_date}.csv\"\n",
    "    if len(contracts_df) > 0:\n",
    "        contracts_df.to_csv(contracts_path, index=False)\n",
    "        logger.info(f\"Saved flagged contracts: {contracts_path.name} ({len(contracts_df)} contracts)\")\n",
    "    \n",
    "    # All scored trades\n",
    "    trades_df = generate_all_flagged_trades(all_results)\n",
    "    trades_path = output_folder / f\"uoa_flagged_trades_{target_date}.parquet\"\n",
    "    if len(trades_df) > 0:\n",
    "        trades_df.to_parquet(trades_path, index=False)\n",
    "        logger.info(f\"Saved all scored trades: {trades_path.name} ({len(trades_df)} trades)\")\n",
    "    \n",
    "    # Run log\n",
    "    processing_time = (datetime.now() - start_time).total_seconds()\n",
    "    run_log = generate_run_log(target_date, all_results, processing_time)\n",
    "    log_path = output_folder / f\"uoa_run_log_{target_date}.json\"\n",
    "    with open(log_path, 'w') as f:\n",
    "        json.dump(run_log, f, indent=2)\n",
    "    logger.info(f\"Saved run log: {log_path.name}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PRINT SUMMARY\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(f\"SUMMARY FOR {target_date}\")\n",
    "    logger.info(\"-\" * 70)\n",
    "    \n",
    "    processed_count = len([r for r in all_results if r['status'] == 'processed'])\n",
    "    filtered_count = len([r for r in all_results if r['status'] == 'filtered'])\n",
    "    skipped_count = len([r for r in all_results if r['status'] == 'skipped'])\n",
    "    \n",
    "    logger.info(f\"Underlyings processed: {processed_count}\")\n",
    "    logger.info(f\"Underlyings filtered (normal day): {filtered_count}\")\n",
    "    logger.info(f\"Underlyings skipped (insufficient data): {skipped_count}\")\n",
    "    \n",
    "    if len(anomaly_df) > 0:\n",
    "        logger.info(f\"\\nTotal anomalies detected: {len(anomaly_df)}\")\n",
    "        logger.info(f\"Unique underlyings with anomalies: {anomaly_df['underlying'].nunique()}\")\n",
    "        logger.info(f\"Total flagged volume: {anomaly_df['size'].sum():,.0f} contracts\")\n",
    "        logger.info(f\"Total flagged notional: ${anomaly_df['opt_trade_notional_value'].sum():,.2f}\")\n",
    "        \n",
    "        logger.info(f\"\\nTop 10 Anomalies:\")\n",
    "        logger.info(\"-\" * 50)\n",
    "        for _, row in anomaly_df.head(10).iterrows():\n",
    "            logger.info(f\"  #{row['rank']}: {row['underlying']} | {row['ticker']} | \"\n",
    "                       f\"Score={row['anomaly_score']:.3f} | \"\n",
    "                       f\"Size={row['size']:,} | \"\n",
    "                       f\"OTM={row['otm_percentage']:.1f}%\")\n",
    "    \n",
    "    logger.info(f\"\\nProcessing time: {processing_time:.1f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        'target_date': target_date,\n",
    "        'status': 'completed',\n",
    "        'underlyings_total': len(underlyings),\n",
    "        'underlyings_processed': processed_count,\n",
    "        'underlyings_filtered': filtered_count,\n",
    "        'underlyings_skipped': skipped_count,\n",
    "        'anomalies_detected': len(anomaly_df),\n",
    "        'processing_time_seconds': processing_time,\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    overall_start_time = datetime.now()\n",
    "    \n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(\"PHASE 04: ANOMALY DETECTION FOR TYPE A UOA\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    input_folder = CONFIG[\"input_folder\"]\n",
    "    output_folder = CONFIG[\"output_folder\"]\n",
    "    single_date_mode = CONFIG[\"single_date_mode\"]\n",
    "    \n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    logger.info(f\"Input folder: {input_folder}\")\n",
    "    logger.info(f\"Output folder: {output_folder}\")\n",
    "    logger.info(f\"Mode: {'Single Date' if single_date_mode else 'Multiple Dates'}\")\n",
    "    logger.info(f\"Training window: {CONFIG['training_window_days']} days\")\n",
    "    logger.info(f\"Minimum history: {CONFIG['min_history_days']} days\")\n",
    "    logger.info(f\"Chain volume z-score threshold: {CONFIG['chain_volume_zscore_threshold']}\")\n",
    "    logger.info(f\"Anomaly percentile: {CONFIG['anomaly_percentile']}%\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # DETERMINE DATES TO PROCESS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if single_date_mode:\n",
    "        target_dates = [CONFIG[\"target_date\"]]\n",
    "        logger.info(f\"Target date: {CONFIG['target_date']}\")\n",
    "    else:\n",
    "        start_date = CONFIG[\"start_date\"]\n",
    "        end_date = CONFIG[\"end_date\"]\n",
    "        target_dates = get_dates_in_range(input_folder, start_date, end_date)\n",
    "        logger.info(f\"Date range: {start_date} to {end_date}\")\n",
    "        logger.info(f\"Found {len(target_dates)} dates with data in range\")\n",
    "    \n",
    "    if not target_dates:\n",
    "        logger.error(\"No dates to process!\")\n",
    "        return\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PROCESS EACH DATE\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    date_results = []\n",
    "    \n",
    "    for i, target_date in enumerate(target_dates):\n",
    "        if not single_date_mode:\n",
    "            logger.info(f\"\\n{'#' * 70}\")\n",
    "            logger.info(f\"# DATE {i+1}/{len(target_dates)}: {target_date}\")\n",
    "            logger.info(f\"{'#' * 70}\")\n",
    "        \n",
    "        result = process_single_date(target_date, input_folder, output_folder)\n",
    "        date_results.append(result)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # OVERALL SUMMARY (for multi-date mode)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if not single_date_mode and len(target_dates) > 1:\n",
    "        overall_time = (datetime.now() - overall_start_time).total_seconds()\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\" * 70)\n",
    "        logger.info(\"OVERALL SUMMARY (ALL DATES)\")\n",
    "        logger.info(\"=\" * 70)\n",
    "        \n",
    "        completed_dates = [r for r in date_results if r['status'] == 'completed']\n",
    "        skipped_dates = [r for r in date_results if r['status'] == 'skipped']\n",
    "        \n",
    "        total_anomalies = sum(r.get('anomalies_detected', 0) for r in completed_dates)\n",
    "        total_processed = sum(r.get('underlyings_processed', 0) for r in completed_dates)\n",
    "        \n",
    "        logger.info(f\"Dates processed: {len(completed_dates)}/{len(target_dates)}\")\n",
    "        logger.info(f\"Dates skipped (no data): {len(skipped_dates)}\")\n",
    "        logger.info(f\"Total anomalies detected: {total_anomalies}\")\n",
    "        logger.info(f\"Total underlyings processed: {total_processed}\")\n",
    "        logger.info(f\"Overall processing time: {overall_time:.1f} seconds\")\n",
    "        \n",
    "        # Save overall summary\n",
    "        def convert_to_native(obj):\n",
    "            \"\"\"Convert numpy types to native Python types for JSON serialization.\"\"\"\n",
    "            if isinstance(obj, (np.integer,)):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, (np.floating,)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_to_native(i) for i in obj]\n",
    "            return obj\n",
    "        \n",
    "        overall_summary = {\n",
    "            'run_timestamp': datetime.now().isoformat(),\n",
    "            'mode': 'multiple_dates',\n",
    "            'start_date': CONFIG['start_date'],\n",
    "            'end_date': CONFIG['end_date'],\n",
    "            'dates_processed': len(completed_dates),\n",
    "            'dates_skipped': len(skipped_dates),\n",
    "            'total_anomalies': int(total_anomalies),\n",
    "            'total_underlyings_processed': int(total_processed),\n",
    "            'overall_processing_time_seconds': round(overall_time, 2),\n",
    "            'date_results': convert_to_native(date_results),\n",
    "        }\n",
    "        \n",
    "        overall_log_path = output_folder / f\"uoa_batch_summary_{CONFIG['start_date']}_to_{CONFIG['end_date']}.json\"\n",
    "        with open(overall_log_path, 'w') as f:\n",
    "            json.dump(overall_summary, f, indent=2)\n",
    "        logger.info(f\"\\nSaved batch summary: {overall_log_path.name}\")\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\" * 70)\n",
    "    logger.info(\"PHASE 05A - Anomaly Detection for LIT COMPLETE\")\n",
    "    logger.info(\"=\" * 70)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
