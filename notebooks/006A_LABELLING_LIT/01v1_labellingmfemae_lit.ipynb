{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PHASE 06A: LABELLING ANOMALIES OF LARGE INFORMED TRADES (LIT)\n",
    "================================================================================\n",
    "\n",
    "Pipeline Position: \n",
    "    Phase 04 (Anomaly Detection) → **Phase 06A** → Phase 06B (Scoring System)\n",
    "\n",
    "Purpose:\n",
    "    Labels each flagged contract and trade with forward-looking return metrics\n",
    "    (MFE, MAE, exit return) so we can later analyze which characteristics\n",
    "    predict profitable signals.\n",
    "\n",
    "Metrics Computed:\n",
    "    - MFE (Maximum Favorable Excursion): Best case profit during holding period\n",
    "    - MAE (Maximum Adverse Excursion): Worst case drawdown during holding period\n",
    "    - Exit Return: Actual return at exit\n",
    "    - MFE/MAE Ratio: Risk-adjusted return potential\n",
    "\n",
    "Trading Rules:\n",
    "    - Entry: Next trading day's OPEN after anomaly detected\n",
    "    - Exit: Exit day's OPEN\n",
    "    - Holding Period: min(DTE - 1, 60 days)\n",
    "    - Direction: CALL → Long underlying, PUT → Short underlying\n",
    "\n",
    "Input:\n",
    "    - Anomalies data: {ANOMALIES_FOLDER}/{TICKER}/uoa_flagged_contracts_YYYY-MM-DD.csv\n",
    "    - Anomalies data: {ANOMALIES_FOLDER}/{TICKER}/uoa_anomalies_YYYY-MM-DD.csv\n",
    "    - Underlying OHLCV: {OHLCV_FOLDER}/YYYY-MM-DD.parquet\n",
    "    \n",
    "Output:\n",
    "    - {OUTPUT_FOLDER}/{TICKER}/uoa_labelled_contracts_YYYY-MM-DD.csv\n",
    "    - {OUTPUT_FOLDER}/{TICKER}/uoa_labelled_anomalies_YYYY-MM-DD.csv\n",
    "\n",
    "Author: [Your Name]\n",
    "Created: 2026-02-XX\n",
    "Version: 1.0\n",
    "\n",
    "Dependencies:\n",
    "    - pandas >= 1.5.0\n",
    "    - numpy >= 1.20.0\n",
    "    - pyarrow >= 10.0.0\n",
    "\n",
    "Usage:\n",
    "    python Phase_06A_labelling_anomalies.py\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # -------------------------------------------------------------------------\n",
    "    # INPUT FOLDERS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Folder containing anomaly detection output (Phase 04)\n",
    "    # Structure: {anomalies_folder}/{TICKER}/uoa_flagged_contracts_YYYY-MM-DD.csv\n",
    "    \"anomalies_folder\": Path(r\"D:\\cyclelabs_codes\\CL_20251120_siphontrades\\01_FIXINGRAWDATA\\output_5a_anomaly\"),\n",
    "    \n",
    "    # Folder containing underlying OHLCV data\n",
    "    # Structure: {ohlcv_folder}/YYYY-MM-DD.parquet\n",
    "    \"ohlcv_folder\": Path(r\"D:\\cyclelabs_codes\\CL_20260116_anomalyfixed\\000_DATA_US_STOCKS_DAY_AGGS\"),\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # OUTPUT FOLDER\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Output folder for labelled data\n",
    "    # Structure: {output_folder}/{TICKER}/uoa_labelled_contracts_YYYY-MM-DD.csv\n",
    "    \"output_folder\": Path(r\"D:\\cyclelabs_codes\\CL_20251120_siphontrades\\01_FIXINGRAWDATA\\output_6a_labelledLITanomalies\"),\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # TICKERS TO PROCESS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # List of tickers to process (modify as needed)\n",
    "    \"tickers_to_process\": [\n",
    "        \"CIFR\",\n",
    "        # \"AAPL\",\n",
    "        # \"TSLA\",\n",
    "        # \"MARA\",\n",
    "        # Add more tickers here\n",
    "    ],\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # DATE RANGE\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    \"start_date\": \"2025-01-01\",\n",
    "    \"end_date\": \"2025-12-31\",\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # TRADING PARAMETERS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Maximum holding period in days\n",
    "    \"max_holding_days\": 60,\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # LOGGING\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    \"log_level\": logging.INFO,\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# LOGGING SETUP\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=CONFIG[\"log_level\"],\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# OHLCV DATA LOADER\n",
    "# =============================================================================\n",
    "\n",
    "class OHLCVLoader:\n",
    "    \"\"\"\n",
    "    Loads and caches underlying OHLCV data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ohlcv_folder: Path):\n",
    "        self.ohlcv_folder = ohlcv_folder\n",
    "        self.cache = {}  # {date_str: DataFrame}\n",
    "        self.available_dates = self._scan_available_dates()\n",
    "        \n",
    "    def _scan_available_dates(self) -> List[str]:\n",
    "        \"\"\"Scan folder for available trading dates.\"\"\"\n",
    "        dates = []\n",
    "        for filepath in self.ohlcv_folder.glob(\"*.parquet\"):\n",
    "            # Filename format: YYYY-MM-DD.parquet\n",
    "            date_str = filepath.stem\n",
    "            try:\n",
    "                datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                dates.append(date_str)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return sorted(dates)\n",
    "    \n",
    "    def get_price_data(self, date_str: str, ticker: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Get OHLCV data for a specific ticker on a specific date.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with keys: open, high, low, close, volume\n",
    "            None if data not available\n",
    "        \"\"\"\n",
    "        if date_str not in self.available_dates:\n",
    "            return None\n",
    "        \n",
    "        # Load from cache or file\n",
    "        if date_str not in self.cache:\n",
    "            filepath = self.ohlcv_folder / f\"{date_str}.parquet\"\n",
    "            try:\n",
    "                df = pd.read_parquet(filepath)\n",
    "                self.cache[date_str] = df\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error loading OHLCV for {date_str}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        df = self.cache[date_str]\n",
    "        \n",
    "        # Filter to ticker\n",
    "        ticker_data = df[df['ticker'] == ticker]\n",
    "        \n",
    "        if len(ticker_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        row = ticker_data.iloc[0]\n",
    "        return {\n",
    "            'open': float(row['open']),\n",
    "            'high': float(row['high']),\n",
    "            'low': float(row['low']),\n",
    "            'close': float(row['close']),\n",
    "            'volume': int(row['volume']),\n",
    "        }\n",
    "    \n",
    "    def get_next_trading_day(self, date_str: str) -> Optional[str]:\n",
    "        \"\"\"Get the next available trading day after date_str.\"\"\"\n",
    "        try:\n",
    "            idx = self.available_dates.index(date_str)\n",
    "            if idx + 1 < len(self.available_dates):\n",
    "                return self.available_dates[idx + 1]\n",
    "        except ValueError:\n",
    "            # date_str not in list, find next available\n",
    "            target_dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "            for d in self.available_dates:\n",
    "                d_dt = datetime.strptime(d, '%Y-%m-%d')\n",
    "                if d_dt > target_dt:\n",
    "                    return d\n",
    "        return None\n",
    "    \n",
    "    def get_trading_day_offset(self, date_str: str, offset: int) -> Optional[str]:\n",
    "        \"\"\"Get trading day that is 'offset' trading days from date_str.\"\"\"\n",
    "        try:\n",
    "            idx = self.available_dates.index(date_str)\n",
    "            target_idx = idx + offset\n",
    "            if 0 <= target_idx < len(self.available_dates):\n",
    "                return self.available_dates[target_idx]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def get_price_range(\n",
    "        self, \n",
    "        ticker: str, \n",
    "        start_date: str, \n",
    "        end_date: str\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get OHLCV data for a ticker between start_date and end_date (inclusive).\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with columns: date, open, high, low, close, volume\n",
    "            None if no data available\n",
    "        \"\"\"\n",
    "        start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        \n",
    "        # Filter available dates to range\n",
    "        dates_in_range = [\n",
    "            d for d in self.available_dates\n",
    "            if start_dt <= datetime.strptime(d, '%Y-%m-%d') <= end_dt\n",
    "        ]\n",
    "        \n",
    "        if not dates_in_range:\n",
    "            return None\n",
    "        \n",
    "        rows = []\n",
    "        for date_str in dates_in_range:\n",
    "            price_data = self.get_price_data(date_str, ticker)\n",
    "            if price_data:\n",
    "                price_data['date'] = date_str\n",
    "                rows.append(price_data)\n",
    "        \n",
    "        if not rows:\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the price data cache to free memory.\"\"\"\n",
    "        self.cache = {}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RETURN METRICS CALCULATOR\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_return_metrics(\n",
    "    ohlcv_loader: OHLCVLoader,\n",
    "    underlying_ticker: str,\n",
    "    anomaly_date: str,\n",
    "    days_to_expiry: int,\n",
    "    option_type: str,  # 'CALL' or 'PUT'\n",
    "    max_holding_days: int = 60\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Calculate MFE, MAE, and exit return for a single anomaly.\n",
    "    \n",
    "    Args:\n",
    "        ohlcv_loader: OHLCVLoader instance\n",
    "        underlying_ticker: The underlying stock ticker\n",
    "        anomaly_date: Date the anomaly was detected (YYYY-MM-DD)\n",
    "        days_to_expiry: Days to expiry of the option\n",
    "        option_type: 'CALL' or 'PUT'\n",
    "        max_holding_days: Maximum holding period\n",
    "        \n",
    "    Returns:\n",
    "        Dict with metrics, or None if calculation not possible\n",
    "    \"\"\"\n",
    "    # Determine direction\n",
    "    direction = 'LONG' if option_type.upper() == 'CALL' else 'SHORT'\n",
    "    \n",
    "    # Calculate holding period\n",
    "    holding_days = min(days_to_expiry - 1, max_holding_days)\n",
    "    \n",
    "    if holding_days <= 0:\n",
    "        return None\n",
    "    \n",
    "    # Get entry date (next trading day after anomaly)\n",
    "    entry_date = ohlcv_loader.get_next_trading_day(anomaly_date)\n",
    "    if not entry_date:\n",
    "        return None\n",
    "    \n",
    "    # Get entry price (open of entry date)\n",
    "    entry_data = ohlcv_loader.get_price_data(entry_date, underlying_ticker)\n",
    "    if not entry_data:\n",
    "        return None\n",
    "    \n",
    "    entry_price = entry_data['open']\n",
    "    \n",
    "    if entry_price <= 0:\n",
    "        return None\n",
    "    \n",
    "    # Get exit date (entry_date + holding_days trading days)\n",
    "    exit_date = ohlcv_loader.get_trading_day_offset(entry_date, holding_days)\n",
    "    if not exit_date:\n",
    "        # If we can't get the exact exit date, use the last available date\n",
    "        # Get price range and use last date\n",
    "        pass\n",
    "    \n",
    "    # Get price data for the entire holding period (inclusive)\n",
    "    price_df = ohlcv_loader.get_price_range(underlying_ticker, entry_date, exit_date if exit_date else entry_date)\n",
    "    \n",
    "    if price_df is None or len(price_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    # If exit_date not found, use last available date in range\n",
    "    if exit_date is None:\n",
    "        exit_date = price_df['date'].iloc[-1]\n",
    "    \n",
    "    # Get exit price (open of exit date)\n",
    "    exit_data = ohlcv_loader.get_price_data(exit_date, underlying_ticker)\n",
    "    if not exit_data:\n",
    "        # Use last row of price_df\n",
    "        exit_price = price_df['open'].iloc[-1]\n",
    "    else:\n",
    "        exit_price = exit_data['open']\n",
    "    \n",
    "    # Calculate max high and min low during holding period\n",
    "    max_high = price_df['high'].max()\n",
    "    min_low = price_df['low'].min()\n",
    "    \n",
    "    # Calculate metrics based on direction\n",
    "    if direction == 'LONG':\n",
    "        # Long position: profit when price goes up\n",
    "        mfe = max_high - entry_price  # Best case: sold at highest high\n",
    "        mae = entry_price - min_low   # Worst case: lowest low\n",
    "        exit_pnl = exit_price - entry_price\n",
    "        \n",
    "        mfe_pct = (mfe / entry_price) * 100\n",
    "        mae_pct = (mae / entry_price) * 100\n",
    "        exit_return_pct = (exit_pnl / entry_price) * 100\n",
    "        \n",
    "    else:  # SHORT\n",
    "        # Short position: profit when price goes down\n",
    "        mfe = entry_price - min_low   # Best case: covered at lowest low\n",
    "        mae = max_high - entry_price  # Worst case: highest high\n",
    "        exit_pnl = entry_price - exit_price\n",
    "        \n",
    "        mfe_pct = (mfe / entry_price) * 100\n",
    "        mae_pct = (mae / entry_price) * 100\n",
    "        exit_return_pct = (exit_pnl / entry_price) * 100\n",
    "    \n",
    "    # Calculate MFE/MAE ratio (handle division by zero)\n",
    "    if mae_pct > 0:\n",
    "        mfe_mae_ratio = mfe_pct / mae_pct\n",
    "    else:\n",
    "        mfe_mae_ratio = np.inf if mfe_pct > 0 else 0\n",
    "    \n",
    "    # Actual holding days (trading days)\n",
    "    actual_holding_days = len(price_df)\n",
    "    \n",
    "    return {\n",
    "        'direction': direction,\n",
    "        'anomaly_date': anomaly_date,\n",
    "        'entry_date': entry_date,\n",
    "        'entry_price': round(entry_price, 4),\n",
    "        'exit_date': exit_date,\n",
    "        'exit_price': round(exit_price, 4),\n",
    "        'planned_holding_days': holding_days,\n",
    "        'actual_holding_days': actual_holding_days,\n",
    "        'max_high': round(max_high, 4),\n",
    "        'min_low': round(min_low, 4),\n",
    "        'mfe': round(mfe, 4),\n",
    "        'mfe_pct': round(mfe_pct, 4),\n",
    "        'mae': round(mae, 4),\n",
    "        'mae_pct': round(mae_pct, 4),\n",
    "        'exit_return': round(exit_pnl, 4),\n",
    "        'exit_return_pct': round(exit_return_pct, 4),\n",
    "        'mfe_mae_ratio': round(mfe_mae_ratio, 4) if mfe_mae_ratio != np.inf else 999.0,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PROCESS FLAGGED CONTRACTS\n",
    "# =============================================================================\n",
    "\n",
    "def process_flagged_contracts(\n",
    "    contracts_df: pd.DataFrame,\n",
    "    underlying_ticker: str,\n",
    "    anomaly_date: str,\n",
    "    ohlcv_loader: OHLCVLoader,\n",
    "    max_holding_days: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process flagged contracts file and add return metrics.\n",
    "    \"\"\"\n",
    "    if len(contracts_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in contracts_df.iterrows():\n",
    "        # Get option type\n",
    "        option_type = row.get('option_type', None)\n",
    "        if option_type is None:\n",
    "            # Try to infer from contract name or other columns\n",
    "            if 'contract' in row:\n",
    "                contract = row['contract']\n",
    "                if 'C' in contract.upper():\n",
    "                    option_type = 'CALL'\n",
    "                elif 'P' in contract.upper():\n",
    "                    option_type = 'PUT'\n",
    "            \n",
    "        if option_type is None:\n",
    "            logger.warning(f\"  Cannot determine option type for row {idx}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Get days to expiry\n",
    "        days_to_expiry = row.get('days_to_expiry', None)\n",
    "        if days_to_expiry is None or pd.isna(days_to_expiry) or days_to_expiry <= 1:\n",
    "            logger.debug(f\"  Invalid DTE for row {idx}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        days_to_expiry = int(days_to_expiry)\n",
    "        \n",
    "        # Calculate return metrics\n",
    "        metrics = calculate_return_metrics(\n",
    "            ohlcv_loader=ohlcv_loader,\n",
    "            underlying_ticker=underlying_ticker,\n",
    "            anomaly_date=anomaly_date,\n",
    "            days_to_expiry=days_to_expiry,\n",
    "            option_type=option_type,\n",
    "            max_holding_days=max_holding_days\n",
    "        )\n",
    "        \n",
    "        if metrics is None:\n",
    "            logger.debug(f\"  Could not calculate metrics for row {idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Combine original row with metrics\n",
    "        result_row = row.to_dict()\n",
    "        result_row.update(metrics)\n",
    "        results.append(result_row)\n",
    "    \n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PROCESS ANOMALIES (TRADE-LEVEL)\n",
    "# =============================================================================\n",
    "\n",
    "def process_anomalies(\n",
    "    anomalies_df: pd.DataFrame,\n",
    "    underlying_ticker: str,\n",
    "    anomaly_date: str,\n",
    "    ohlcv_loader: OHLCVLoader,\n",
    "    max_holding_days: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process anomalies file (trade-level) and add return metrics.\n",
    "    \"\"\"\n",
    "    if len(anomalies_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in anomalies_df.iterrows():\n",
    "        # Get option type\n",
    "        option_type = row.get('option_type', None)\n",
    "        \n",
    "        # Try alternative column names\n",
    "        if option_type is None:\n",
    "            if 'option_type_call' in row:\n",
    "                option_type = 'CALL' if row['option_type_call'] == 1 else 'PUT'\n",
    "        \n",
    "        if option_type is None:\n",
    "            # Try to infer from ticker/contract name\n",
    "            ticker = row.get('ticker', '')\n",
    "            if 'C' in str(ticker).upper()[-10:]:  # Check last part of ticker\n",
    "                option_type = 'CALL'\n",
    "            elif 'P' in str(ticker).upper()[-10:]:\n",
    "                option_type = 'PUT'\n",
    "        \n",
    "        if option_type is None:\n",
    "            logger.debug(f\"  Cannot determine option type for row {idx}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Get days to expiry\n",
    "        days_to_expiry = row.get('days_to_expiry', None)\n",
    "        if days_to_expiry is None or pd.isna(days_to_expiry) or days_to_expiry <= 1:\n",
    "            logger.debug(f\"  Invalid DTE for row {idx}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        days_to_expiry = int(days_to_expiry)\n",
    "        \n",
    "        # Calculate return metrics\n",
    "        metrics = calculate_return_metrics(\n",
    "            ohlcv_loader=ohlcv_loader,\n",
    "            underlying_ticker=underlying_ticker,\n",
    "            anomaly_date=anomaly_date,\n",
    "            days_to_expiry=days_to_expiry,\n",
    "            option_type=option_type,\n",
    "            max_holding_days=max_holding_days\n",
    "        )\n",
    "        \n",
    "        if metrics is None:\n",
    "            logger.debug(f\"  Could not calculate metrics for row {idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Combine original row with metrics\n",
    "        result_row = row.to_dict()\n",
    "        result_row.update(metrics)\n",
    "        results.append(result_row)\n",
    "    \n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def get_anomaly_files_for_ticker(\n",
    "    anomalies_folder: Path,\n",
    "    ticker: str,\n",
    "    start_date: str,\n",
    "    end_date: str\n",
    ") -> List[Tuple[str, Path, Path]]:\n",
    "    \"\"\"\n",
    "    Get all anomaly files for a ticker within the date range.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (date_str, contracts_filepath, anomalies_filepath)\n",
    "    \"\"\"\n",
    "    ticker_folder = anomalies_folder / ticker\n",
    "    \n",
    "    if not ticker_folder.exists():\n",
    "        return []\n",
    "    \n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    # Find all flagged_contracts files\n",
    "    files = []\n",
    "    for contracts_file in ticker_folder.glob(\"uoa_flagged_contracts_*.csv\"):\n",
    "        # Extract date from filename\n",
    "        filename = contracts_file.stem  # uoa_flagged_contracts_YYYY-MM-DD\n",
    "        date_str = filename.replace(\"uoa_flagged_contracts_\", \"\")\n",
    "        \n",
    "        try:\n",
    "            file_dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        # Check if within date range\n",
    "        if not (start_dt <= file_dt <= end_dt):\n",
    "            continue\n",
    "        \n",
    "        # Check for corresponding anomalies file\n",
    "        anomalies_file = ticker_folder / f\"uoa_anomalies_{date_str}.csv\"\n",
    "        \n",
    "        files.append((date_str, contracts_file, anomalies_file))\n",
    "    \n",
    "    # Sort by date\n",
    "    files = sorted(files, key=lambda x: x[0])\n",
    "    \n",
    "    return files\n",
    "\n",
    "\n",
    "def process_ticker(\n",
    "    ticker: str,\n",
    "    anomalies_folder: Path,\n",
    "    ohlcv_loader: OHLCVLoader,\n",
    "    output_folder: Path,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    max_holding_days: int\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Process all anomaly files for a single ticker.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing ticker: {ticker}\")\n",
    "    \n",
    "    # Get all files for this ticker\n",
    "    files = get_anomaly_files_for_ticker(anomalies_folder, ticker, start_date, end_date)\n",
    "    \n",
    "    if not files:\n",
    "        logger.info(f\"  No anomaly files found for {ticker}\")\n",
    "        return {\n",
    "            'ticker': ticker,\n",
    "            'files_processed': 0,\n",
    "            'contracts_labelled': 0,\n",
    "            'anomalies_labelled': 0,\n",
    "        }\n",
    "    \n",
    "    logger.info(f\"  Found {len(files)} anomaly files\")\n",
    "    \n",
    "    # Create output folder for ticker\n",
    "    ticker_output_folder = output_folder / ticker\n",
    "    ticker_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    total_contracts_labelled = 0\n",
    "    total_anomalies_labelled = 0\n",
    "    files_processed = 0\n",
    "    \n",
    "    for date_str, contracts_file, anomalies_file in files:\n",
    "        logger.info(f\"  Processing {date_str}...\")\n",
    "        \n",
    "        # Process flagged contracts\n",
    "        if contracts_file.exists():\n",
    "            try:\n",
    "                contracts_df = pd.read_csv(contracts_file)\n",
    "                \n",
    "                if len(contracts_df) > 0:\n",
    "                    labelled_contracts = process_flagged_contracts(\n",
    "                        contracts_df=contracts_df,\n",
    "                        underlying_ticker=ticker,\n",
    "                        anomaly_date=date_str,\n",
    "                        ohlcv_loader=ohlcv_loader,\n",
    "                        max_holding_days=max_holding_days\n",
    "                    )\n",
    "                    \n",
    "                    if len(labelled_contracts) > 0:\n",
    "                        output_path = ticker_output_folder / f\"uoa_labelled_contracts_{date_str}.csv\"\n",
    "                        labelled_contracts.to_csv(output_path, index=False)\n",
    "                        total_contracts_labelled += len(labelled_contracts)\n",
    "                        logger.info(f\"    Labelled {len(labelled_contracts)} contracts\")\n",
    "                    else:\n",
    "                        logger.info(f\"    No contracts could be labelled\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"    Error processing contracts: {e}\")\n",
    "        \n",
    "        # Process anomalies (trade-level)\n",
    "        if anomalies_file.exists():\n",
    "            try:\n",
    "                anomalies_df = pd.read_csv(anomalies_file)\n",
    "                \n",
    "                if len(anomalies_df) > 0:\n",
    "                    labelled_anomalies = process_anomalies(\n",
    "                        anomalies_df=anomalies_df,\n",
    "                        underlying_ticker=ticker,\n",
    "                        anomaly_date=date_str,\n",
    "                        ohlcv_loader=ohlcv_loader,\n",
    "                        max_holding_days=max_holding_days\n",
    "                    )\n",
    "                    \n",
    "                    if len(labelled_anomalies) > 0:\n",
    "                        output_path = ticker_output_folder / f\"uoa_labelled_anomalies_{date_str}.csv\"\n",
    "                        labelled_anomalies.to_csv(output_path, index=False)\n",
    "                        total_anomalies_labelled += len(labelled_anomalies)\n",
    "                        logger.info(f\"    Labelled {len(labelled_anomalies)} anomalies (trade-level)\")\n",
    "                    else:\n",
    "                        logger.info(f\"    No anomalies could be labelled\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"    Error processing anomalies: {e}\")\n",
    "        \n",
    "        files_processed += 1\n",
    "        \n",
    "        # Clear OHLCV cache periodically to manage memory\n",
    "        if files_processed % 10 == 0:\n",
    "            ohlcv_loader.clear_cache()\n",
    "    \n",
    "    return {\n",
    "        'ticker': ticker,\n",
    "        'files_processed': files_processed,\n",
    "        'contracts_labelled': total_contracts_labelled,\n",
    "        'anomalies_labelled': total_anomalies_labelled,\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(\"PHASE 06A: LABELLING ANOMALIES OF LARGE INFORMED TRADES\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    anomalies_folder = CONFIG[\"anomalies_folder\"]\n",
    "    ohlcv_folder = CONFIG[\"ohlcv_folder\"]\n",
    "    output_folder = CONFIG[\"output_folder\"]\n",
    "    tickers_to_process = CONFIG[\"tickers_to_process\"]\n",
    "    start_date = CONFIG[\"start_date\"]\n",
    "    end_date = CONFIG[\"end_date\"]\n",
    "    max_holding_days = CONFIG[\"max_holding_days\"]\n",
    "    \n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    logger.info(f\"Anomalies folder: {anomalies_folder}\")\n",
    "    logger.info(f\"OHLCV folder: {ohlcv_folder}\")\n",
    "    logger.info(f\"Output folder: {output_folder}\")\n",
    "    logger.info(f\"Tickers to process: {tickers_to_process}\")\n",
    "    logger.info(f\"Date range: {start_date} to {end_date}\")\n",
    "    logger.info(f\"Max holding days: {max_holding_days}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # INITIALIZE OHLCV LOADER\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(\"INITIALIZING OHLCV LOADER\")\n",
    "    logger.info(\"-\" * 70)\n",
    "    \n",
    "    ohlcv_loader = OHLCVLoader(ohlcv_folder)\n",
    "    logger.info(f\"Found {len(ohlcv_loader.available_dates)} trading days in OHLCV data\")\n",
    "    \n",
    "    if len(ohlcv_loader.available_dates) > 0:\n",
    "        logger.info(f\"Date range: {ohlcv_loader.available_dates[0]} to {ohlcv_loader.available_dates[-1]}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PROCESS EACH TICKER\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    logger.info(\"-\" * 70)\n",
    "    logger.info(\"PROCESSING TICKERS\")\n",
    "    logger.info(\"-\" * 70)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, ticker in enumerate(tickers_to_process):\n",
    "        logger.info(f\"\\n[{i+1}/{len(tickers_to_process)}] {ticker}\")\n",
    "        \n",
    "        result = process_ticker(\n",
    "            ticker=ticker,\n",
    "            anomalies_folder=anomalies_folder,\n",
    "            ohlcv_loader=ohlcv_loader,\n",
    "            output_folder=output_folder,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            max_holding_days=max_holding_days\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SUMMARY\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    processing_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\" * 70)\n",
    "    logger.info(\"SUMMARY\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    total_files = sum(r['files_processed'] for r in all_results)\n",
    "    total_contracts = sum(r['contracts_labelled'] for r in all_results)\n",
    "    total_anomalies = sum(r['anomalies_labelled'] for r in all_results)\n",
    "    \n",
    "    logger.info(f\"Tickers processed: {len(tickers_to_process)}\")\n",
    "    logger.info(f\"Total files processed: {total_files}\")\n",
    "    logger.info(f\"Total contracts labelled: {total_contracts}\")\n",
    "    logger.info(f\"Total anomalies labelled: {total_anomalies}\")\n",
    "    logger.info(f\"Processing time: {processing_time:.1f} seconds\")\n",
    "    \n",
    "    # Per-ticker summary\n",
    "    logger.info(\"\\nPer-Ticker Summary:\")\n",
    "    logger.info(\"-\" * 50)\n",
    "    for r in all_results:\n",
    "        logger.info(f\"  {r['ticker']}: {r['files_processed']} files, \"\n",
    "                   f\"{r['contracts_labelled']} contracts, \"\n",
    "                   f\"{r['anomalies_labelled']} anomalies\")\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\" * 70)\n",
    "    logger.info(\"PHASE 06A COMPLETE\")\n",
    "    logger.info(\"=\" * 70)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
